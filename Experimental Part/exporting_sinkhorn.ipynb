{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from dataclasses import replace\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import struct\n",
    "import optax as ox\n",
    "\n",
    "# Packages that actually performs Sinkhorn algorithm\n",
    "from ott.geometry.pointcloud import PointCloud\n",
    "from ott.problems.linear.linear_problem import LinearProblem\n",
    "from ott.solvers.linear.sinkhorn import Sinkhorn\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.gaussian_process import kernels\n",
    "## Preprocessing step\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random # to select randomly the reference distribution\n",
    "\n",
    "import h5py\n",
    "#import plotly.graph_objects as go\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def read_cgns_coordinates(file_path):\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        # We retrieve coordinate by coordinate.\n",
    "        # ! Notice the space before the data. This is due to the naming in the files themselves.\n",
    "        x = np.array(file['Base_2_3/Zone/GridCoordinates/CoordinateX'].get(' data'))\n",
    "        y = np.array(file['Base_2_3/Zone/GridCoordinates/CoordinateY'].get(' data'))\n",
    "        z = np.array(file['Base_2_3/Zone/GridCoordinates/CoordinateZ'].get(' data'))\n",
    "\n",
    "    return x, y, z\n",
    "\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class WeightedPointCloud:\n",
    "  \"\"\"A weighted point cloud.\n",
    "  \n",
    "  Attributes:\n",
    "    cloud: Array of shape (n, d) where n is the number of points and d the dimension.\n",
    "    weights: Array of shape (n,) where n is the number of points.\n",
    "  \"\"\"\n",
    "  cloud: jnp.array\n",
    "  weights: jnp.array\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.cloud.shape[0]\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class VectorizedWeightedPointCloud:\n",
    "  \"\"\"Vectorized version of WeightedPointCloud.\n",
    "\n",
    "  Assume that b clouds are all of size n and dimension d.\n",
    "  \n",
    "  Attributes:\n",
    "    _private_cloud: Array of shape (b, n, d) where n is the number of points and d the dimension.\n",
    "    _private_weights: Array of shape (b, n) where n is the number of points.\n",
    "  \n",
    "  Methods:\n",
    "    unpack: returns the cloud and weights.\n",
    "  \"\"\"\n",
    "  _private_cloud: jnp.array\n",
    "  _private_weights: jnp.array\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return WeightedPointCloud(self._private_cloud[idx], self._private_cloud[idx])\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self._private_cloud.shape[0]\n",
    "  \n",
    "  def __iter__(self):\n",
    "    for i in range(len(self)):\n",
    "      yield self[i]\n",
    "\n",
    "  def unpack(self):\n",
    "    return self._private_cloud, self._private_weights\n",
    "\n",
    "def pad_point_cloud(point_cloud, max_cloud_size, fail_on_too_big=True):\n",
    "  \"\"\"Pad a single point cloud with zeros to have the same size.\n",
    "  \n",
    "  Args:\n",
    "    point_cloud: a weighted point cloud.\n",
    "    max_cloud_size: the size of the biggest point cloud.\n",
    "    fail_on_too_big: if True, raise an error if the cloud is too big for padding.\n",
    "  \n",
    "  Returns:\n",
    "    a WeightedPointCloud with padded cloud and weights.\n",
    "  \"\"\"\n",
    "  cloud, weights = point_cloud.cloud, point_cloud.weights\n",
    "  delta = max_cloud_size - cloud.shape[0]\n",
    "  if delta <= 0:\n",
    "    if fail_on_too_big:\n",
    "      assert False, 'Cloud is too big for padding.'\n",
    "    return point_cloud\n",
    "\n",
    "  ratio = 1e-3  # less than 0.1% of the total mass.\n",
    "  smallest_weight = jnp.min(weights) / delta * ratio\n",
    "  small_weights = jnp.ones(delta) * smallest_weight\n",
    "\n",
    "  weights = weights * (1 - ratio)  # keep 99.9% of the mass.\n",
    "  weights = jnp.concatenate([weights, small_weights], axis=0)\n",
    "\n",
    "  cloud = jnp.pad(cloud, pad_width=((0, delta), (0,0)), mode='mean')\n",
    "\n",
    "  point_cloud = WeightedPointCloud(cloud, weights)\n",
    "\n",
    "  return point_cloud\n",
    "\n",
    "def pad_point_clouds(cloud_list):\n",
    "  \"\"\"Pad the point clouds with zeros to have the same size.\n",
    "\n",
    "  Note: this function should be used outside of jax.jit because the computation graph\n",
    "        is huge. O(len(cloud_list)) nodes are generated.\n",
    "\n",
    "  Args:\n",
    "    cloud_list: a list of WeightedPointCloud.\n",
    "  \n",
    "  Returns:\n",
    "    a VectrorizedWeightedPointCloud with padded clouds and weights.\n",
    "  \"\"\"\n",
    "  # sentinel for unified processing of all clouds, including biggest one.\n",
    "  max_cloud_size = max([len(cloud) for cloud in cloud_list]) + 1\n",
    "  sentinel_padder = partial(pad_point_cloud, max_cloud_size=max_cloud_size)\n",
    "\n",
    "  cloud_list = list(map(sentinel_padder, cloud_list))\n",
    "  coordinates = jnp.stack([cloud.cloud for cloud in cloud_list])\n",
    "  weights = jnp.stack([cloud.weights for cloud in cloud_list])\n",
    "  return VectorizedWeightedPointCloud(coordinates, weights)\n",
    "\n",
    "def clouds_barycenter(points):\n",
    "  \"\"\"Compute the barycenter of a set of clouds.\n",
    "  \n",
    "  Args:\n",
    "    points: a VectorizedWeightedPointCloud.\n",
    "    \n",
    "  Returns:\n",
    "    a barycenter of the clouds of points, of shape (1, d) where d is the dimension.\n",
    "  \"\"\"\n",
    "  clouds, weights = points.unpack()\n",
    "  barycenter = jnp.sum(clouds * weights[:,:,jnp.newaxis], axis=1)\n",
    "  barycenter = jnp.mean(barycenter, axis=0, keepdims=True)\n",
    "  return barycenter\n",
    "\n",
    "\n",
    "def to_simplex(mu):\n",
    "  \"\"\"Project weights to the simplex.\n",
    "  \n",
    "  Args: \n",
    "    mu: a WeightedPointCloud.\n",
    "    \n",
    "  Returns:\n",
    "    a WeightedPointCloud with weights projected to the simplex.\"\"\"\n",
    "  if mu.weights is None:\n",
    "    mu_weights = None\n",
    "  else:\n",
    "    mu_weights = jax.nn.softmax(mu.weights)\n",
    "  return replace(mu, weights=mu_weights)\n",
    "\n",
    "\n",
    "def reparametrize_mu(mu, cloud_barycenter, scale):\n",
    "  \"\"\"Re-parametrize mu to be invariant by translation and scaling.\n",
    "\n",
    "  Args:\n",
    "    mu: a WeightedPointCloud.\n",
    "    cloud_barycenter: Array of shape (1, d) where d is the dimension.\n",
    "    scale: float, scaling parameter for the re-parametrization of mu.\n",
    "  \n",
    "  Returns:\n",
    "    a WeightedPointCloud with re-parametrized weights and cloud.\n",
    "  \"\"\"\n",
    "  # invariance by translation : recenter mu around its mean\n",
    "  mu_cloud = mu.cloud - jnp.mean(mu.cloud, axis=0, keepdims=True)  # center.\n",
    "  mu_cloud = scale * jnp.tanh(mu_cloud)  # re-parametrization of the domain.\n",
    "  mu_cloud = mu_cloud + cloud_barycenter  # re-center toward barycenter of all clouds.\n",
    "  return replace(mu, cloud=mu_cloud)\n",
    "\n",
    "\n",
    "def clouds_to_dual_sinkhorn(points, \n",
    "                            mu, \n",
    "                            init_dual=(None, None),\n",
    "                            scale=1.,\n",
    "                            has_aux=False,\n",
    "                            sinkhorn_solver_kwargs=None, \n",
    "                            parallel: bool = True,\n",
    "                            batch_size: int = -1):\n",
    "  \"\"\"Compute the embeddings of the clouds with regularized OT towards mu.\n",
    "  \n",
    "  Args:\n",
    "    points: a VectorizedWeightedPointCloud.\n",
    "    init_dual: tuple of two arrays of shape (b, n) and (b, m) where b is the number of clouds,\n",
    "               n is the number of points in each cloud, and m the number of points in mu.\n",
    "    scale: float, scaling parameter for the re-parametrization of mu.\n",
    "    has_aux: bool, whether to return the full Sinkhorn output or only the dual variables.\n",
    "    sinkhorn_solver_kwargs: dict, kwargs for the Sinkhorn solver.\n",
    "      Must contain the key 'epsilon' for the regularization parameter.\n",
    "\n",
    "  Returns:\n",
    "    a tuple (dual, init_dual) with dual variables of shape (n, m) where n is the number of points\n",
    "    and m the number of points in mu, and init_dual a tuple (init_dual_cloud, init_dual_mu) \n",
    "  \"\"\"\n",
    "  sinkhorn_epsilon = sinkhorn_solver_kwargs.pop('epsilon')\n",
    "  \n",
    "  # weight projection\n",
    "  barycenter = clouds_barycenter(points)\n",
    "  mu = to_simplex(mu)\n",
    "\n",
    "  # cloud projection\n",
    "  mu = reparametrize_mu(mu, barycenter, scale)\n",
    "\n",
    "  def sinkhorn_single_cloud(cloud, weights, init_dual):\n",
    "    geom = PointCloud(cloud, mu.cloud,\n",
    "                      epsilon=sinkhorn_epsilon)\n",
    "    ot_prob = LinearProblem(geom,\n",
    "                            weights,\n",
    "                            mu.weights)\n",
    "    solver = Sinkhorn(**sinkhorn_solver_kwargs)\n",
    "    ot = solver(ot_prob, init=init_dual)\n",
    "    return ot\n",
    "  \n",
    "  if parallel:\n",
    "    if batch_size == -1:\n",
    "        parallel_sinkhorn = jax.vmap(sinkhorn_single_cloud,\n",
    "                                    in_axes=(0, 0, (0, 0)),\n",
    "                                    out_axes=0)\n",
    "        outs = parallel_sinkhorn(*points.unpack(), init_dual)\n",
    "        return outs.g\n",
    "    else:\n",
    "      raise ValueError(\"Not coded yet\") \n",
    "  else:\n",
    "    list_of_g_potentials = []\n",
    "    clouds, weights = points.unpack()\n",
    "    for i in range(len(clouds)):\n",
    "      ot_problem = sinkhorn_single_cloud(clouds[i], weights[i], init_dual)\n",
    "      list_of_g_potentials.append(ot_problem.g)\n",
    "    g_potentials_array = jnp.stack(list_of_g_potentials)\n",
    "    return g_potentials_array\n",
    "  \n",
    "\n",
    "# Set the seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of blade one want to consider. We consider the train_250 split.\n",
    "blades_number_train = [3,6,7,16,20,21,22,23,29,33,34,39,46,56,57,71,76,77,78,81,83,95,99,101,102,105,115,117,124,130,143,145,152,154,157,159,160,167,173,174,180,182,187,190,196,198,201,203,204,210,212,217,220,223,224,229,233,246,247,250,251,252,264,268,270,278,288,289,300,312,314,316,317,319,320,324,334,335,337,339,348,356,357,359,367,369,370,371,375,376,377,379,383,389,395,396,398,400,404,405,408,413,414,415,416,420,426,428,431,435,436,441,443,444,449,452,463,468,469,471,472,479,483,490,501,512,513,516,518,519,523,524,525,526,527,528,530,532,553,556,557,558,561,567,568,570,572,573,575,589,593,595,597,601,606,612,616,621,622,624,628,629,631,638,641,643,647,648,657,662,663,673,677,681,692,699,703,704,705,711,713,715,721,728,731,732,741,742,747,754,757,760,763,766,769,772,779,781,782,783,784,798,800,806,812,813,816,823,826,832,833,834,836,842,843,846,852,854,857,864,866,871,872,876,877,884,892,896,901,909,922,927,931,936,937,939,946,956,959,965,975,978,982,985,987,993,994,995,996,999]\n",
    "blades_number_test = [1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199]\n",
    "\n",
    "## Creating the list of all file numbers.\n",
    "padded_numbers_train = [str(i).zfill(9) for i in blades_number_train]\n",
    "padded_numbers_test = [str(i).zfill(9) for i in blades_number_train]\n",
    "\n",
    "## Lists that will holds the cloud points and the associated efficiency.\n",
    "distributions_train = []\n",
    "distributions_test = []\n",
    "\n",
    "\n",
    "for number in padded_numbers_train:\n",
    "    ## File paths Personal Computer\n",
    "    cgns_file_path = f'Rotor37/dataset/samples/sample_{number}/meshes/mesh_000000000.cgns'\n",
    "    ## Computing the coordinates\n",
    "    x, y, z = read_cgns_coordinates(cgns_file_path)\n",
    "    blade = np.column_stack((x, y, z))\n",
    "    ## Adding to our data\n",
    "    distributions_train.append(blade)\n",
    "\n",
    "# Test\n",
    "for number in padded_numbers_test:\n",
    "    ## File paths Personal Computer\n",
    "    cgns_file_path = f'Rotor37/dataset/samples/sample_{number}/meshes/mesh_000000000.cgns'\n",
    "    ## Computing the coordinates\n",
    "    x, y, z = read_cgns_coordinates(cgns_file_path)\n",
    "    blade = np.column_stack((x, y, z))\n",
    "    ## Adding to our data\n",
    "    distributions_test.append(blade)\n",
    "\n",
    "#### Here the reference measure is the same for train and test\n",
    "mu = random.choice(distributions_train)\n",
    "## Convert the list of points mu into a WeightedPointCloud object\n",
    "mu_cloud = WeightedPointCloud(\n",
    "    cloud=jnp.array(mu),\n",
    "    weights=jnp.ones(len(mu))\n",
    ")\n",
    "\n",
    "\n",
    "## First we convert the list all the sampled distributions to WeightedPointCloud objects\n",
    "list_of_weighted_point_clouds_train = []\n",
    "for sample in distributions_train:\n",
    "    distrib_cloud = WeightedPointCloud(\n",
    "        cloud=jnp.array(sample),\n",
    "        weights=jnp.ones(len(sample)))\n",
    "    list_of_weighted_point_clouds_train.append(distrib_cloud)\n",
    "\n",
    "# Test\n",
    "list_of_weighted_point_clouds_test = []\n",
    "for sample in distributions_test:\n",
    "    distrib_cloud = WeightedPointCloud(\n",
    "        cloud=jnp.array(sample),\n",
    "        weights=jnp.ones(len(sample)))\n",
    "    list_of_weighted_point_clouds_test.append(distrib_cloud)\n",
    "\n",
    "## We need to convert the cloud list to a VectorizedWeightedPointCloud\n",
    "x_cloud_train = pad_point_clouds(list_of_weighted_point_clouds_train)\n",
    "x_cloud_test = pad_point_clouds(list_of_weighted_point_clouds_test)\n",
    "\n",
    "## We choose our epsilon parameter and perform the sinkhirn algorithm\n",
    "sinkhorn_solver_kwargs = {'epsilon': 0.01}\n",
    "# Train\n",
    "sinkhorn_potentials_train = clouds_to_dual_sinkhorn(points = x_cloud_train, mu = mu_cloud, \n",
    "                                                   sinkhorn_solver_kwargs = sinkhorn_solver_kwargs, \n",
    "                                                   parallel = False, # going into the for loop\n",
    "                                                   batch_size = -1)\n",
    "np.savetxt(\"sinkhorn_potentials_train_250_epsilon_001.csv\", sinkhorn_potentials_train, delimiter=\";\")\n",
    "\n",
    "# Test\n",
    "sinkhorn_potentials_test = clouds_to_dual_sinkhorn(points = x_cloud_test, mu = mu_cloud, \n",
    "                                                   sinkhorn_solver_kwargs = sinkhorn_solver_kwargs, \n",
    "                                                   parallel = False, # going into the for loop\n",
    "                                                   batch_size = -1)\n",
    "np.savetxt(\"sinkhorn_potentials_test_epsilon_001.csv\", sinkhorn_potentials_test, delimiter=\";\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
