{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from dataclasses import replace\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import struct\n",
    "import optax as ox\n",
    "\n",
    "# Packages that actually performs Sinkhorn algorithm\n",
    "from ott.geometry.pointcloud import PointCloud\n",
    "from ott.problems.linear.linear_problem import LinearProblem\n",
    "from ott.solvers.linear.sinkhorn import Sinkhorn\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.gaussian_process import kernels\n",
    "## Preprocessing step\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import h5py\n",
    "#import plotly.graph_objects as go\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def read_cgns_coordinates(file_path):\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        # We retrieve coordinate by coordinate.\n",
    "        # ! Notice the space before the data. This is due to the naming in the files themselves.\n",
    "        x = np.array(file['Base_2_3/Zone/GridCoordinates/CoordinateX'].get(' data'))\n",
    "        y = np.array(file['Base_2_3/Zone/GridCoordinates/CoordinateY'].get(' data'))\n",
    "        z = np.array(file['Base_2_3/Zone/GridCoordinates/CoordinateZ'].get(' data'))\n",
    "\n",
    "    return x, y, z\n",
    "\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class WeightedPointCloud:\n",
    "  \"\"\"A weighted point cloud.\n",
    "  \n",
    "  Attributes:\n",
    "    cloud: Array of shape (n, d) where n is the number of points and d the dimension.\n",
    "    weights: Array of shape (n,) where n is the number of points.\n",
    "  \"\"\"\n",
    "  cloud: jnp.array\n",
    "  weights: jnp.array\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.cloud.shape[0]\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class VectorizedWeightedPointCloud:\n",
    "  \"\"\"Vectorized version of WeightedPointCloud.\n",
    "\n",
    "  Assume that b clouds are all of size n and dimension d.\n",
    "  \n",
    "  Attributes:\n",
    "    _private_cloud: Array of shape (b, n, d) where n is the number of points and d the dimension.\n",
    "    _private_weights: Array of shape (b, n) where n is the number of points.\n",
    "  \n",
    "  Methods:\n",
    "    unpack: returns the cloud and weights.\n",
    "  \"\"\"\n",
    "  _private_cloud: jnp.array\n",
    "  _private_weights: jnp.array\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return WeightedPointCloud(self._private_cloud[idx], self._private_cloud[idx])\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self._private_cloud.shape[0]\n",
    "  \n",
    "  def __iter__(self):\n",
    "    for i in range(len(self)):\n",
    "      yield self[i]\n",
    "\n",
    "  def unpack(self):\n",
    "    return self._private_cloud, self._private_weights\n",
    "\n",
    "def pad_point_cloud(point_cloud, max_cloud_size, fail_on_too_big=True):\n",
    "  \"\"\"Pad a single point cloud with zeros to have the same size.\n",
    "  \n",
    "  Args:\n",
    "    point_cloud: a weighted point cloud.\n",
    "    max_cloud_size: the size of the biggest point cloud.\n",
    "    fail_on_too_big: if True, raise an error if the cloud is too big for padding.\n",
    "  \n",
    "  Returns:\n",
    "    a WeightedPointCloud with padded cloud and weights.\n",
    "  \"\"\"\n",
    "  cloud, weights = point_cloud.cloud, point_cloud.weights\n",
    "  delta = max_cloud_size - cloud.shape[0]\n",
    "  if delta <= 0:\n",
    "    if fail_on_too_big:\n",
    "      assert False, 'Cloud is too big for padding.'\n",
    "    return point_cloud\n",
    "\n",
    "  ratio = 1e-3  # less than 0.1% of the total mass.\n",
    "  smallest_weight = jnp.min(weights) / delta * ratio\n",
    "  small_weights = jnp.ones(delta) * smallest_weight\n",
    "\n",
    "  weights = weights * (1 - ratio)  # keep 99.9% of the mass.\n",
    "  weights = jnp.concatenate([weights, small_weights], axis=0)\n",
    "\n",
    "  cloud = jnp.pad(cloud, pad_width=((0, delta), (0,0)), mode='mean')\n",
    "\n",
    "  point_cloud = WeightedPointCloud(cloud, weights)\n",
    "\n",
    "  return point_cloud\n",
    "\n",
    "def pad_point_clouds(cloud_list):\n",
    "  \"\"\"Pad the point clouds with zeros to have the same size.\n",
    "\n",
    "  Note: this function should be used outside of jax.jit because the computation graph\n",
    "        is huge. O(len(cloud_list)) nodes are generated.\n",
    "\n",
    "  Args:\n",
    "    cloud_list: a list of WeightedPointCloud.\n",
    "  \n",
    "  Returns:\n",
    "    a VectrorizedWeightedPointCloud with padded clouds and weights.\n",
    "  \"\"\"\n",
    "  # sentinel for unified processing of all clouds, including biggest one.\n",
    "  max_cloud_size = max([len(cloud) for cloud in cloud_list]) + 1\n",
    "  sentinel_padder = partial(pad_point_cloud, max_cloud_size=max_cloud_size)\n",
    "\n",
    "  cloud_list = list(map(sentinel_padder, cloud_list))\n",
    "  coordinates = jnp.stack([cloud.cloud for cloud in cloud_list])\n",
    "  weights = jnp.stack([cloud.weights for cloud in cloud_list])\n",
    "  return VectorizedWeightedPointCloud(coordinates, weights)\n",
    "\n",
    "def clouds_barycenter(points):\n",
    "  \"\"\"Compute the barycenter of a set of clouds.\n",
    "  \n",
    "  Args:\n",
    "    points: a VectorizedWeightedPointCloud.\n",
    "    \n",
    "  Returns:\n",
    "    a barycenter of the clouds of points, of shape (1, d) where d is the dimension.\n",
    "  \"\"\"\n",
    "  clouds, weights = points.unpack()\n",
    "  barycenter = jnp.sum(clouds * weights[:,:,jnp.newaxis], axis=1)\n",
    "  barycenter = jnp.mean(barycenter, axis=0, keepdims=True)\n",
    "  return barycenter\n",
    "\n",
    "\n",
    "def to_simplex(mu):\n",
    "  \"\"\"Project weights to the simplex.\n",
    "  \n",
    "  Args: \n",
    "    mu: a WeightedPointCloud.\n",
    "    \n",
    "  Returns:\n",
    "    a WeightedPointCloud with weights projected to the simplex.\"\"\"\n",
    "  if mu.weights is None:\n",
    "    mu_weights = None\n",
    "  else:\n",
    "    mu_weights = jax.nn.softmax(mu.weights)\n",
    "  return replace(mu, weights=mu_weights)\n",
    "\n",
    "\n",
    "def reparametrize_mu(mu, cloud_barycenter, scale):\n",
    "  \"\"\"Re-parametrize mu to be invariant by translation and scaling.\n",
    "\n",
    "  Args:\n",
    "    mu: a WeightedPointCloud.\n",
    "    cloud_barycenter: Array of shape (1, d) where d is the dimension.\n",
    "    scale: float, scaling parameter for the re-parametrization of mu.\n",
    "  \n",
    "  Returns:\n",
    "    a WeightedPointCloud with re-parametrized weights and cloud.\n",
    "  \"\"\"\n",
    "  # invariance by translation : recenter mu around its mean\n",
    "  mu_cloud = mu.cloud - jnp.mean(mu.cloud, axis=0, keepdims=True)  # center.\n",
    "  mu_cloud = scale * jnp.tanh(mu_cloud)  # re-parametrization of the domain.\n",
    "  mu_cloud = mu_cloud + cloud_barycenter  # re-center toward barycenter of all clouds.\n",
    "  return replace(mu, cloud=mu_cloud)\n",
    "\n",
    "\n",
    "def clouds_to_dual_sinkhorn(points, \n",
    "                            mu, \n",
    "                            init_dual=(None, None),\n",
    "                            scale=1.,\n",
    "                            has_aux=False,\n",
    "                            sinkhorn_solver_kwargs=None, \n",
    "                            parallel: bool = True,\n",
    "                            batch_size: int = -1):\n",
    "  \"\"\"Compute the embeddings of the clouds with regularized OT towards mu.\n",
    "  \n",
    "  Args:\n",
    "    points: a VectorizedWeightedPointCloud.\n",
    "    init_dual: tuple of two arrays of shape (b, n) and (b, m) where b is the number of clouds,\n",
    "               n is the number of points in each cloud, and m the number of points in mu.\n",
    "    scale: float, scaling parameter for the re-parametrization of mu.\n",
    "    has_aux: bool, whether to return the full Sinkhorn output or only the dual variables.\n",
    "    sinkhorn_solver_kwargs: dict, kwargs for the Sinkhorn solver.\n",
    "      Must contain the key 'epsilon' for the regularization parameter.\n",
    "\n",
    "  Returns:\n",
    "    a tuple (dual, init_dual) with dual variables of shape (n, m) where n is the number of points\n",
    "    and m the number of points in mu, and init_dual a tuple (init_dual_cloud, init_dual_mu) \n",
    "  \"\"\"\n",
    "  sinkhorn_epsilon = sinkhorn_solver_kwargs.pop('epsilon')\n",
    "  \n",
    "  # weight projection\n",
    "  barycenter = clouds_barycenter(points)\n",
    "  mu = to_simplex(mu)\n",
    "\n",
    "  # cloud projection\n",
    "  mu = reparametrize_mu(mu, barycenter, scale)\n",
    "\n",
    "  def sinkhorn_single_cloud(cloud, weights, init_dual):\n",
    "    geom = PointCloud(cloud, mu.cloud,\n",
    "                      epsilon=sinkhorn_epsilon)\n",
    "    ot_prob = LinearProblem(geom,\n",
    "                            weights,\n",
    "                            mu.weights)\n",
    "    solver = Sinkhorn(**sinkhorn_solver_kwargs)\n",
    "    ot = solver(ot_prob, init=init_dual)\n",
    "    return ot\n",
    "  \n",
    "  if parallel:\n",
    "    if batch_size == -1:\n",
    "        parallel_sinkhorn = jax.vmap(sinkhorn_single_cloud,\n",
    "                                    in_axes=(0, 0, (0, 0)),\n",
    "                                    out_axes=0)\n",
    "        outs = parallel_sinkhorn(*points.unpack(), init_dual)\n",
    "        return outs.g\n",
    "    else:\n",
    "      raise ValueError(\"Not coded yet\") \n",
    "  else:\n",
    "    list_of_g_potentials = []\n",
    "    clouds, weights = points.unpack()\n",
    "    for i in range(len(clouds)):\n",
    "      ot_problem = sinkhorn_single_cloud(clouds[i], weights[i], init_dual)\n",
    "      list_of_g_potentials.append(ot_problem.g)\n",
    "    g_potentials_array = jnp.stack(list_of_g_potentials)\n",
    "    return g_potentials_array\n",
    "  \n",
    "\n",
    "# Set the seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of blade one want to consider.\n",
    "_many_blades = 50\n",
    "\n",
    "## Creating the list of all file numbers.\n",
    "padded_numbers = [str(i).zfill(9) for i in range(_many_blades)]\n",
    "\n",
    "## Lists that will holds the cloud points and the associated efficiency.\n",
    "distributions = []\n",
    "efficiency = []\n",
    "omega = []\n",
    "P = []\n",
    "\n",
    "for number in padded_numbers:\n",
    "    ## File paths Google Colab\n",
    "    #cgns_file_path = f'/content/drive/MyDrive/Developer/GraduationProject/Experimental Part/Rotor37/dataset/samples/sample_{number}/meshes/mesh_000000000.cgns'\n",
    "    #coefficient_file_path = f'/content/drive/MyDrive/Developer/GraduationProject/Experimental Part/Rotor37/dataset/samples/sample_{number}/scalars.csv'\n",
    "    ## File paths Personal Computer\n",
    "    cgns_file_path = f'Rotor37/dataset/samples/sample_{number}/meshes/mesh_000000000.cgns'\n",
    "    coefficient_file_path = f'Rotor37/dataset/samples/sample_{number}/scalars.csv'\n",
    "    ## Computing the coordinates\n",
    "    x, y, z = read_cgns_coordinates(cgns_file_path)\n",
    "    blade = np.column_stack((x, y, z))\n",
    "    ## Computing the coefficient\n",
    "    scalars = pd.read_csv(coefficient_file_path)\n",
    "    ## Adding to our data\n",
    "    distributions.append(blade)\n",
    "    efficiency.append(scalars[\"Efficiency\"][0])\n",
    "    omega.append(scalars[\"Omega\"][0])\n",
    "    P.append(scalars[\"P\"][0])\n",
    "\n",
    "\n",
    "## Use array objects and reshape them to have the correct form. This is important for the preprocessing steps later.\n",
    "omega = np.array(omega)\n",
    "omega = omega.reshape(-1, 1)\n",
    "\n",
    "P = np.array(P)\n",
    "P = P.reshape(-1, 1)\n",
    "\n",
    "efficiency = np.array(efficiency)\n",
    "efficiency = efficiency.reshape(-1, 1)\n",
    "\n",
    "mu = distributions[0]\n",
    "\n",
    "sinkhorn_potentials = pd.read_csv(\"sinkhorn_potentials_50.csv\", sep = \";\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 35)\n",
      "(15, 35)\n"
     ]
    }
   ],
   "source": [
    "feature_matrix = np.hstack((sinkhorn_potentials, P, omega))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_matrix, efficiency, train_size = 0.7, random_state = 42)\n",
    "\n",
    "## Define the kernels for different parts of the feature matrix\n",
    "kernel_sinkhorn = kernels.RBF()\n",
    "kernel_scalars = kernels.RBF(length_scale = np.array([1, 1])) # anisotropic kernel\n",
    "\n",
    "sinkhorn_train = x_train[: , 0:len(mu)]\n",
    "scalars_train = x_train[: , len(mu):]\n",
    "\n",
    "# Train the normalizer on the train data\n",
    "normalize = StandardScaler().fit(scalars_train)\n",
    "scalars_train = normalize.transform(scalars_train)\n",
    "\n",
    "## Kernels matrices\n",
    "kernel_matrix_sinkhorn_train = kernel_sinkhorn(sinkhorn_train)\n",
    "kernel_matrix_scalars_train = kernel_scalars(scalars_train)\n",
    "k_train = kernel_matrix_sinkhorn_train*kernel_matrix_scalars_train\n",
    "print(k_train.shape)\n",
    "\n",
    "sinkhorn_test = x_test[: , 0:len(mu)]\n",
    "scalars_test = x_test[: , len(mu):]\n",
    "# Apply the normalizer on the test data\n",
    "scalars_test = normalize.transform(scalars_test)\n",
    "## Kernels matrices\n",
    "kernel_matrix_sinkhorn_test = kernel_sinkhorn(sinkhorn_test, sinkhorn_train)\n",
    "kernel_matrix_scalars_test = kernel_scalars(scalars_test, scalars_train)\n",
    "k_test = kernel_matrix_sinkhorn_test*kernel_matrix_scalars_test\n",
    "print(k_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'alpha': 0.001}\n",
      "Mean Square Error on the Test Set: 0.7166156821348948\n",
      "Explained Variance Score on the Test Set: -165.96731306542324\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Per-column arrays must each be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[1;32m     40\u001b[0m sns\u001b[38;5;241m.\u001b[39mset_theme(context\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper\u001b[39m\u001b[38;5;124m'\u001b[39m, font_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatterplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m## Adding the x=y line and the text\u001b[39;00m\n\u001b[1;32m     44\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([\u001b[38;5;28mmin\u001b[39m(y_test), \u001b[38;5;28mmax\u001b[39m(y_test)], [\u001b[38;5;28mmin\u001b[39m(y_test), \u001b[38;5;28mmax\u001b[39m(y_test)], linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m)\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-louis.allain@gmail.com/My Drive/Developer/GraduationProject/.venv/lib/python3.12/site-packages/seaborn/relational.py:615\u001b[0m, in \u001b[0;36mscatterplot\u001b[0;34m(data, x, y, hue, size, style, palette, hue_order, hue_norm, sizes, size_order, size_norm, markers, style_order, legend, ax, **kwargs)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatterplot\u001b[39m(\n\u001b[1;32m    607\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    608\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, hue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    613\u001b[0m ):\n\u001b[0;32m--> 615\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43m_ScatterPlotter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstyle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlegend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegend\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_hue(palette\u001b[38;5;241m=\u001b[39mpalette, order\u001b[38;5;241m=\u001b[39mhue_order, norm\u001b[38;5;241m=\u001b[39mhue_norm)\n\u001b[1;32m    622\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_size(sizes\u001b[38;5;241m=\u001b[39msizes, order\u001b[38;5;241m=\u001b[39msize_order, norm\u001b[38;5;241m=\u001b[39msize_norm)\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-louis.allain@gmail.com/My Drive/Developer/GraduationProject/.venv/lib/python3.12/site-packages/seaborn/relational.py:396\u001b[0m, in \u001b[0;36m_ScatterPlotter.__init__\u001b[0;34m(self, data, variables, legend)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, variables\u001b[38;5;241m=\u001b[39m{}, legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    388\u001b[0m \n\u001b[1;32m    389\u001b[0m     \u001b[38;5;66;03m# TODO this is messy, we want the mapping to be agnostic about\u001b[39;00m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;66;03m# the kind of plot to draw, but for the time being we need to set\u001b[39;00m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;66;03m# this information so the SizeMapping can use it\u001b[39;00m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_size_range \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    393\u001b[0m         np\u001b[38;5;241m.\u001b[39mr_[\u001b[38;5;241m.5\u001b[39m, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msquare(mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlines.markersize\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    394\u001b[0m     )\n\u001b[0;32m--> 396\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegend \u001b[38;5;241m=\u001b[39m legend\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-louis.allain@gmail.com/My Drive/Developer/GraduationProject/.venv/lib/python3.12/site-packages/seaborn/_base.py:634\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_ordered \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# alt., used DefaultDict\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# TODO Lots of tests assume that these are called to initialize the\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# mappings to default values on class initialization. I'd prefer to\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# move away from that and only have a mapping when explicitly called.\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-louis.allain@gmail.com/My Drive/Developer/GraduationProject/.venv/lib/python3.12/site-packages/seaborn/_base.py:679\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;66;03m# When dealing with long-form input, use the newer PlotData\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;66;03m# object (internal but introduced for the objects interface)\u001b[39;00m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;66;03m# to centralize / standardize data consumption logic.\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 679\u001b[0m     plot_data \u001b[38;5;241m=\u001b[39m \u001b[43mPlotData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m     frame \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mframe\n\u001b[1;32m    681\u001b[0m     names \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mnames\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-louis.allain@gmail.com/My Drive/Developer/GraduationProject/.venv/lib/python3.12/site-packages/seaborn/_core/data.py:58\u001b[0m, in \u001b[0;36mPlotData.__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     53\u001b[0m     data: DataSource,\n\u001b[1;32m     54\u001b[0m     variables: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, VariableSpec],\n\u001b[1;32m     55\u001b[0m ):\n\u001b[1;32m     57\u001b[0m     data \u001b[38;5;241m=\u001b[39m handle_data_source(data)\n\u001b[0;32m---> 58\u001b[0m     frame, names, ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe \u001b[38;5;241m=\u001b[39m frame\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m names\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-louis.allain@gmail.com/My Drive/Developer/GraduationProject/.venv/lib/python3.12/site-packages/seaborn/_core/data.py:265\u001b[0m, in \u001b[0;36mPlotData._assign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    260\u001b[0m             ids[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mid\u001b[39m(val)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Construct a tidy plot DataFrame. This will convert a number of\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# types automatically, aligning on index in case of pandas objects\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# TODO Note: this fails when variable specs *only* have scalars!\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplot_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frame, names, ids\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-louis.allain@gmail.com/My Drive/Developer/GraduationProject/.venv/lib/python3.12/site-packages/pandas/core/frame.py:767\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    761\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    762\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    763\u001b[0m     )\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-louis.allain@gmail.com/My Drive/Developer/GraduationProject/.venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-louis.allain@gmail.com/My Drive/Developer/GraduationProject/.venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-louis.allain@gmail.com/My Drive/Developer/GraduationProject/.venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:664\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    662\u001b[0m         raw_lengths\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(val))\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m val\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 664\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Per-column arrays must each be 1-dimensional"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## We perform the Kernel Ridge Regression\n",
    "krr = KernelRidge(kernel = \"precomputed\")\n",
    "\n",
    "## Define the parameter grid\n",
    "## We can only optimize on the regularization parameter because we use a precomputed kernel.\n",
    "param_grid = {'alpha': [0.001, 0.01, 0.1, 0.5, 1, 10, 100]}\n",
    "\n",
    "## Create GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator = krr, \n",
    "                           param_grid = param_grid, \n",
    "                           scoring = 'neg_mean_squared_error', \n",
    "                           cv = 5)\n",
    "\n",
    "## Fit the model ie training the model\n",
    "grid_search.fit(X = k_train, y = y_train)\n",
    "\n",
    "## Get the best parameters from the cross validation\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "## Get the best model\n",
    "my_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "## Obtain predictions for the test set\n",
    "predictions = my_model.predict(X = k_test)\n",
    "\n",
    "## Compute the MSE\n",
    "mse = mean_squared_error(y_true = y_test, y_pred = predictions)\n",
    "\n",
    "# Compute the EVS\n",
    "evs = explained_variance_score(y_true = y_test, y_pred = predictions)\n",
    "\n",
    "# Print the MSE and EVS\n",
    "print(f'Mean Square Error on the Test Set: {mse}')\n",
    "print(f'Explained Variance Score on the Test Set: {evs}')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "sns.set_theme(context='paper', font_scale=1.5)\n",
    "sns.scatterplot(x=y_test, y=predictions, color='blue', alpha=0.5)\n",
    "\n",
    "## Adding the x=y line and the text\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2, alpha = 0.8)\n",
    "plt.text(1, 1, f'EVS = {evs:.3f}', ha='left', va='top', color='black', fontsize=10, weight='bold')\n",
    "\n",
    "plt.title('Predicted vs. Actual Values of Y')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "\n",
    "#plt.savefig('Images/regression_toy_experiment.png', dpi=300, bbox_inches='tight',format=\"png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the features matrix\n",
    "feature_matrix = np.hstack((sinkhorn_potentials, P, omega))\n",
    "feature_matrix_wout_pot = np.hstack((P, omega))\n",
    "feature_matrix_only_pot = sinkhorn_potentials\n",
    "\n",
    "\n",
    "## Define the kernels for different parts of the feature matrix\n",
    "kernel_sinkhorn = kernels.RBF()\n",
    "kernel_scalars = kernels.RBF(length_scale = np.array([1, 1])) # anisotropic kernel\n",
    "\n",
    "## Kernels matrices\n",
    "kernel_matrix_sinkhorn = kernel_sinkhorn(feature_matrix_only_pot)\n",
    "kernel_matrix_scalars = kernel_scalars(feature_matrix_wout_pot)\n",
    "\n",
    "kernel_matrix = kernel_matrix_sinkhorn*kernel_matrix_scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_matrix_only_pot.shape)\n",
    "print(feature_matrix_wout_pot.shape)\n",
    "print(feature_matrix.shape)\n",
    "\n",
    "print(kernel_matrix_sinkhorn.shape)\n",
    "print(kernel_matrix_scalars.shape)\n",
    "\n",
    "print(kernel_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[: , len(mu):]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
