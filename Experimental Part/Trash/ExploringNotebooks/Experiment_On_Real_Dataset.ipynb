{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment on a real dataset\n",
    "\n",
    "This notebook follows the reproduction of the paper's experiment. Here, we apply the Sinkhirn kernel on a real dataset provided [by Safran here](https://plaid-lib.readthedocs.io/en/latest/source/data_challenges/rotor37.html). The data is composed of 3D modelisations of motor blades. Those blades are represented by a cloud points. \n",
    "\n",
    "The goal of this notebook is to perform regression task with X variable being the cloud points of the blade (one cloud point = 1 blade = 1 observation) and the target variable Y is a aerodynamic coefficient provided with the metadata of the blade. \n",
    "- We are going to carry this task using the Sinkhorn kernel of our [paper of interest](https://doi.org/10.48550/arXiv.2210.06574). \n",
    "- After that, we aim at using other kernel to carry out Kernel Ridge Regression, namely the Wassestein kernel and the Sliced Wasserstein kernel, and compare accuracy and computation efficiency of those different methods against the Sinkhorn kernel. \n",
    "- If time allows, the goal is to perform this regression task using Gaussian Processes, again with multiple kernels.\n",
    "\n",
    "<br>\n",
    "\n",
    "Authors of this notebook:\n",
    "* Louis Allain\n",
    "* LÃ©onard Gousset\n",
    "* Julien Heurtin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Sinkhorn kernel to perform Kernel Ridge Regression\n",
    "\n",
    "The **first** problem that arises is the computation time. Each blade is made of about $30,000$ points. Performing Regularized Optimal Transport on such a big distribution's sample is long. Moreover, the function used in the toy experiment previoulsy uses the ```clouds_to_dual_sinkhorn``` function to perform ROT between two cloud points. This function uses a ```jax.vmap``` to perform the computation. Unfortunately this function has a very high usage of memory (which is needed to perform the computations in parallel) and is therefore not viable for our problem's size. Multiple solutions to bypass this issue:\n",
    "1) Undersample every blade. Instead of a blade being $30,000$, we randomly select a portion of those points.\n",
    "2) Undersample the training set. Instead of using the full $1,200$ blades we consider only a fraction of that. This is made possible beacuse the dataset has multiple train sample sizes.\n",
    "3) Changing the way the computation is done. ```jax.vmap``` is very time effective. We could sacrifice time for memory, for instance by computing the ROT sequentially rather than parallely.\n",
    "\n",
    "The **second** question is *which reference measure to consider ?* Again multiple ideas come to mind:\n",
    "1) Using the first blade as the reference measure.\n",
    "2) Taking the smallest rectangle in which all the blades fit and sampling points on its borders.\n",
    "3) Taking random points in approximately a good range?\n",
    "\n",
    "Many questions are still un-answered:\n",
    "1) How many points should make the reference measure ?\n",
    "2) For points 3., what should be the variance of the distribution ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the Kernel Ridge Regression from scratch\n",
    "\n",
    "**DO NOT WORK WELL**\n",
    "\n",
    "The functions from the paper normalized the reference measure. Is it necessary here ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_barycenter(points_list):\n",
    "    total_points = 0\n",
    "    sum_x = 0\n",
    "    sum_y = 0\n",
    "    sum_z = 0\n",
    "    \n",
    "    # Iterate over each set of points\n",
    "    for points in points_list:\n",
    "        num_points = len(points)\n",
    "        if num_points == 0:\n",
    "            raise ValueError(\"Cannot calculate the barycenter of an empty set of points.\")\n",
    "        \n",
    "        # Sum the coordinates along each axis for this set of points\n",
    "        sum_x += np.sum(points[:, 0])\n",
    "        sum_y += np.sum(points[:, 1])\n",
    "        sum_z += np.sum(points[:, 2])\n",
    "        \n",
    "        total_points += num_points\n",
    "    \n",
    "    # Calculate the global barycenter coordinates\n",
    "    barycenter_x = sum_x / total_points\n",
    "    barycenter_y = sum_y / total_points\n",
    "    barycenter_z = sum_z / total_points\n",
    "    \n",
    "    return barycenter_x, barycenter_y, barycenter_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data and train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "\n",
    "## Number of blade one want to consider.\n",
    "_many_blades = 10\n",
    "\n",
    "## Creating the list of all file numbers.\n",
    "padded_numbers = [str(i).zfill(9) for i in range(_many_blades)]\n",
    "\n",
    "## Lists that will holds the cloud points and the associated efficiency.\n",
    "distributions = []\n",
    "efficiency = []\n",
    "\n",
    "for number in padded_numbers:\n",
    "    ## File paths Google Colab\n",
    "    #cgns_file_path = f'/content/drive/MyDrive/Developer/GraduationProject/Experimental Part/Rotor37/dataset/samples/sample_{number}/meshes/mesh_000000000.cgns'\n",
    "    #coefficient_file_path = f'/content/drive/MyDrive/Developer/GraduationProject/Experimental Part/Rotor37/dataset/samples/sample_{number}/scalars.csv'\n",
    "    ## File paths Personal Computer\n",
    "    cgns_file_path = f'Rotor37/dataset/samples/sample_{number}/meshes/mesh_000000000.cgns'\n",
    "    coefficient_file_path = f'Rotor37/dataset/samples/sample_{number}/scalars.csv'\n",
    "    ## Computing the coordinates\n",
    "    x, y, z = read_cgns_coordinates(cgns_file_path)\n",
    "    blade = np.column_stack((x, y, z))\n",
    "    ## Computing the coefficient\n",
    "    scalars = pd.read_csv(coefficient_file_path)\n",
    "    ## Adding to our data\n",
    "    distributions.append(blade)\n",
    "    efficiency.append(scalars[\"Efficiency\"][0])\n",
    "\n",
    "## Train Test split of 70%\n",
    "x_train, x_test, y_train, y_test = train_test_split(distributions, efficiency, train_size = 0.7, random_state = 42)\n",
    "\n",
    "## Transforming the train data to jnp.array\n",
    "clouds_train = jnp.array(x_train)\n",
    "clouds_test = jnp.array(x_test)\n",
    "\n",
    "## Computing the barycentre of all clouds\n",
    "x_barycenter, y_barycenter, z_barycenter = global_barycenter(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the reference measure which depends on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sampling the sphere of a given radius and of center the barycenter of the blade\n",
    "num_points = 1000\n",
    "radius = 0.04\n",
    "\n",
    "## Sampling the reference measure. Note that the barycenter depends of the training data\n",
    "ref_x, ref_y, ref_z = sample_points_on_sphere(num_points, radius = radius, center = (x_barycenter, y_barycenter, z_barycenter))\n",
    "reference_measure_sample = np.column_stack((ref_x, ref_y, ref_z))\n",
    "\n",
    "## Using jnp.array objects\n",
    "cloud_ref = jnp.array(reference_measure_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the potentials for each ROT problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_potentials = []\n",
    "\n",
    "## Define the epsilon for Regularized OT - small epsilon = great accuracy = longer time\n",
    "epsilon = 1\n",
    "\n",
    "for cloud in clouds_train:\n",
    "\n",
    "    ## Create PointCloud object to accomodate OTT package\n",
    "    my_geom = PointCloud(x = cloud, # training cloud\n",
    "                         y = cloud_ref, # reference cloud\n",
    "                         epsilon = epsilon # epsilon of ROT\n",
    "                         )\n",
    "\n",
    "    ## Formalises the Regularized Optimal Transport problem\n",
    "    ot_problem = LinearProblem(geom = my_geom) # The geometry of the problem between the two blades\n",
    "\n",
    "    ## Instanciate the solver of the ROT problem\n",
    "    sinkhorn_solver =  Sinkhorn()\n",
    "\n",
    "    ## Actually computing the Sinkhorn algortihm\n",
    "    rot_result = sinkhorn_solver(ot_problem)\n",
    "    \n",
    "    ## Retrieve RIGHT potentials (we are only interested in those)\n",
    "    right_potentials = rot_result.g\n",
    "\n",
    "    x_train_potentials.append(right_potentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We perform the Kernel Ridge Regression\n",
    "krr = KernelRidge(kernel = \"rbf\")\n",
    "\n",
    "## Define the parameter grid\n",
    "param_grid = {'alpha': [0.01, 0.1, 1, 10], 'gamma': [0.01, 0.1, 1, 10]}\n",
    "\n",
    "## Create GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator = krr, \n",
    "                           param_grid = param_grid, \n",
    "                           scoring = 'neg_mean_squared_error', \n",
    "                           cv = 5)\n",
    "\n",
    "## Fit the model ie training the model\n",
    "grid_search.fit(X = x_train_potentials, y = y_train)\n",
    "\n",
    "## Get the best parameters from the cross validation\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "## Get the best model\n",
    "my_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing ROT on test data and testing the Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_potentials = []\n",
    "\n",
    "## Define the epsilon for Regularized OT - small epsilon = great accuracy = longer time\n",
    "epsilon = 1\n",
    "\n",
    "for cloud in clouds_test:\n",
    "\n",
    "    ## Create PointCloud object to accomodate OTT package\n",
    "    my_geom = PointCloud(x = cloud, # training cloud\n",
    "                         y = cloud_ref, # reference cloud\n",
    "                         epsilon = epsilon # epsilon of ROT\n",
    "                         )\n",
    "\n",
    "    ## Formalises the Regularized Optimal Transport problem\n",
    "    ot_problem = LinearProblem(geom = my_geom) # The geometry of the problem between the two blades\n",
    "\n",
    "    ## Instanciate the solver of the ROT problem\n",
    "    sinkhorn_solver =  Sinkhorn()\n",
    "\n",
    "    ## Actually computing the Sinkhorn algortihm\n",
    "    rot_result = sinkhorn_solver(ot_problem)\n",
    "    \n",
    "    ## Retrieve RIGHT potentials (we are only interested in those)\n",
    "    right_potentials = rot_result.g\n",
    "\n",
    "    x_test_potentials.append(right_potentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtain predictions for the test set\n",
    "predictions = my_model.predict(X = x_test_potentials)\n",
    "\n",
    "## Compute the MSE\n",
    "mse = mean_squared_error(y_true = y_test, y_pred = predictions)\n",
    "\n",
    "# Compute the EVS\n",
    "evs = explained_variance_score(y_true = y_test, y_pred = predictions)\n",
    "\n",
    "# Print the MSE and EVS\n",
    "print(f'Mean Square Error on the Test Set: {mse}')\n",
    "print(f'Explained Variance Score on the Test Set: {evs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "sns.set_theme(context='paper', font_scale=1.5)\n",
    "sns.scatterplot(x=y_test, y=predictions, color='blue', alpha=0.5)\n",
    "\n",
    "## Adding the x=y line and the text\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2, alpha = 0.8)\n",
    "plt.text(min(y_test), max(predictions), f'EVS = {evs:.3f}', ha='left', va='top', color='black', fontsize=10, weight='bold')\n",
    "\n",
    "plt.title('Predicted vs. Actual Values of Y')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "\n",
    "plt.savefig('Images/regression_500_blades.png', dpi=300, bbox_inches='tight',format=\"png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the Kernel Ridge Regression with the paper's functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing functions and packages and setting the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nx/gcjsd2hn49l_4ypjlnw3m0d00000gn/T/ipykernel_1563/1537333419.py:23: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from dataclasses import replace\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import struct\n",
    "import optax as ox\n",
    "\n",
    "# Packages that actually performs Sinkhorn algorithm\n",
    "from ott.geometry.pointcloud import PointCloud\n",
    "from ott.problems.linear.linear_problem import LinearProblem\n",
    "from ott.solvers.linear.sinkhorn import Sinkhorn\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.gaussian_process import kernels\n",
    "## Preprocessing step\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import h5py\n",
    "#import plotly.graph_objects as go\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def read_cgns_coordinates(file_path):\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        # We retrieve coordinate by coordinate.\n",
    "        # ! Notice the space before the data. This is due to the naming in the files themselves.\n",
    "        x = np.array(file['Base_2_3/Zone/GridCoordinates/CoordinateX'].get(' data'))\n",
    "        y = np.array(file['Base_2_3/Zone/GridCoordinates/CoordinateY'].get(' data'))\n",
    "        z = np.array(file['Base_2_3/Zone/GridCoordinates/CoordinateZ'].get(' data'))\n",
    "\n",
    "    return x, y, z\n",
    "\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class WeightedPointCloud:\n",
    "  \"\"\"A weighted point cloud.\n",
    "  \n",
    "  Attributes:\n",
    "    cloud: Array of shape (n, d) where n is the number of points and d the dimension.\n",
    "    weights: Array of shape (n,) where n is the number of points.\n",
    "  \"\"\"\n",
    "  cloud: jnp.array\n",
    "  weights: jnp.array\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.cloud.shape[0]\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class VectorizedWeightedPointCloud:\n",
    "  \"\"\"Vectorized version of WeightedPointCloud.\n",
    "\n",
    "  Assume that b clouds are all of size n and dimension d.\n",
    "  \n",
    "  Attributes:\n",
    "    _private_cloud: Array of shape (b, n, d) where n is the number of points and d the dimension.\n",
    "    _private_weights: Array of shape (b, n) where n is the number of points.\n",
    "  \n",
    "  Methods:\n",
    "    unpack: returns the cloud and weights.\n",
    "  \"\"\"\n",
    "  _private_cloud: jnp.array\n",
    "  _private_weights: jnp.array\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return WeightedPointCloud(self._private_cloud[idx], self._private_cloud[idx])\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self._private_cloud.shape[0]\n",
    "  \n",
    "  def __iter__(self):\n",
    "    for i in range(len(self)):\n",
    "      yield self[i]\n",
    "\n",
    "  def unpack(self):\n",
    "    return self._private_cloud, self._private_weights\n",
    "\n",
    "def pad_point_cloud(point_cloud, max_cloud_size, fail_on_too_big=True):\n",
    "  \"\"\"Pad a single point cloud with zeros to have the same size.\n",
    "  \n",
    "  Args:\n",
    "    point_cloud: a weighted point cloud.\n",
    "    max_cloud_size: the size of the biggest point cloud.\n",
    "    fail_on_too_big: if True, raise an error if the cloud is too big for padding.\n",
    "  \n",
    "  Returns:\n",
    "    a WeightedPointCloud with padded cloud and weights.\n",
    "  \"\"\"\n",
    "  cloud, weights = point_cloud.cloud, point_cloud.weights\n",
    "  delta = max_cloud_size - cloud.shape[0]\n",
    "  if delta <= 0:\n",
    "    if fail_on_too_big:\n",
    "      assert False, 'Cloud is too big for padding.'\n",
    "    return point_cloud\n",
    "\n",
    "  ratio = 1e-3  # less than 0.1% of the total mass.\n",
    "  smallest_weight = jnp.min(weights) / delta * ratio\n",
    "  small_weights = jnp.ones(delta) * smallest_weight\n",
    "\n",
    "  weights = weights * (1 - ratio)  # keep 99.9% of the mass.\n",
    "  weights = jnp.concatenate([weights, small_weights], axis=0)\n",
    "\n",
    "  cloud = jnp.pad(cloud, pad_width=((0, delta), (0,0)), mode='mean')\n",
    "\n",
    "  point_cloud = WeightedPointCloud(cloud, weights)\n",
    "\n",
    "  return point_cloud\n",
    "\n",
    "def pad_point_clouds(cloud_list):\n",
    "  \"\"\"Pad the point clouds with zeros to have the same size.\n",
    "\n",
    "  Note: this function should be used outside of jax.jit because the computation graph\n",
    "        is huge. O(len(cloud_list)) nodes are generated.\n",
    "\n",
    "  Args:\n",
    "    cloud_list: a list of WeightedPointCloud.\n",
    "  \n",
    "  Returns:\n",
    "    a VectrorizedWeightedPointCloud with padded clouds and weights.\n",
    "  \"\"\"\n",
    "  # sentinel for unified processing of all clouds, including biggest one.\n",
    "  max_cloud_size = max([len(cloud) for cloud in cloud_list]) + 1\n",
    "  sentinel_padder = partial(pad_point_cloud, max_cloud_size=max_cloud_size)\n",
    "\n",
    "  cloud_list = list(map(sentinel_padder, cloud_list))\n",
    "  coordinates = jnp.stack([cloud.cloud for cloud in cloud_list])\n",
    "  weights = jnp.stack([cloud.weights for cloud in cloud_list])\n",
    "  return VectorizedWeightedPointCloud(coordinates, weights)\n",
    "\n",
    "def clouds_barycenter(points):\n",
    "  \"\"\"Compute the barycenter of a set of clouds.\n",
    "  \n",
    "  Args:\n",
    "    points: a VectorizedWeightedPointCloud.\n",
    "    \n",
    "  Returns:\n",
    "    a barycenter of the clouds of points, of shape (1, d) where d is the dimension.\n",
    "  \"\"\"\n",
    "  clouds, weights = points.unpack()\n",
    "  barycenter = jnp.sum(clouds * weights[:,:,jnp.newaxis], axis=1)\n",
    "  barycenter = jnp.mean(barycenter, axis=0, keepdims=True)\n",
    "  return barycenter\n",
    "\n",
    "\n",
    "def to_simplex(mu):\n",
    "  \"\"\"Project weights to the simplex.\n",
    "  \n",
    "  Args: \n",
    "    mu: a WeightedPointCloud.\n",
    "    \n",
    "  Returns:\n",
    "    a WeightedPointCloud with weights projected to the simplex.\"\"\"\n",
    "  if mu.weights is None:\n",
    "    mu_weights = None\n",
    "  else:\n",
    "    mu_weights = jax.nn.softmax(mu.weights)\n",
    "  return replace(mu, weights=mu_weights)\n",
    "\n",
    "\n",
    "def reparametrize_mu(mu, cloud_barycenter, scale):\n",
    "  \"\"\"Re-parametrize mu to be invariant by translation and scaling.\n",
    "\n",
    "  Args:\n",
    "    mu: a WeightedPointCloud.\n",
    "    cloud_barycenter: Array of shape (1, d) where d is the dimension.\n",
    "    scale: float, scaling parameter for the re-parametrization of mu.\n",
    "  \n",
    "  Returns:\n",
    "    a WeightedPointCloud with re-parametrized weights and cloud.\n",
    "  \"\"\"\n",
    "  # invariance by translation : recenter mu around its mean\n",
    "  mu_cloud = mu.cloud - jnp.mean(mu.cloud, axis=0, keepdims=True)  # center.\n",
    "  mu_cloud = scale * jnp.tanh(mu_cloud)  # re-parametrization of the domain.\n",
    "  mu_cloud = mu_cloud + cloud_barycenter  # re-center toward barycenter of all clouds.\n",
    "  return replace(mu, cloud=mu_cloud)\n",
    "\n",
    "\n",
    "def clouds_to_dual_sinkhorn(points, \n",
    "                            mu, \n",
    "                            init_dual=(None, None),\n",
    "                            scale=1.,\n",
    "                            has_aux=False,\n",
    "                            sinkhorn_solver_kwargs=None, \n",
    "                            parallel: bool = True,\n",
    "                            batch_size: int = -1):\n",
    "  \"\"\"Compute the embeddings of the clouds with regularized OT towards mu.\n",
    "  \n",
    "  Args:\n",
    "    points: a VectorizedWeightedPointCloud.\n",
    "    init_dual: tuple of two arrays of shape (b, n) and (b, m) where b is the number of clouds,\n",
    "               n is the number of points in each cloud, and m the number of points in mu.\n",
    "    scale: float, scaling parameter for the re-parametrization of mu.\n",
    "    has_aux: bool, whether to return the full Sinkhorn output or only the dual variables.\n",
    "    sinkhorn_solver_kwargs: dict, kwargs for the Sinkhorn solver.\n",
    "      Must contain the key 'epsilon' for the regularization parameter.\n",
    "\n",
    "  Returns:\n",
    "    a tuple (dual, init_dual) with dual variables of shape (n, m) where n is the number of points\n",
    "    and m the number of points in mu, and init_dual a tuple (init_dual_cloud, init_dual_mu) \n",
    "  \"\"\"\n",
    "  sinkhorn_epsilon = sinkhorn_solver_kwargs.pop('epsilon')\n",
    "  \n",
    "  # weight projection\n",
    "  barycenter = clouds_barycenter(points)\n",
    "  mu = to_simplex(mu)\n",
    "\n",
    "  # cloud projection\n",
    "  mu = reparametrize_mu(mu, barycenter, scale)\n",
    "\n",
    "  def sinkhorn_single_cloud(cloud, weights, init_dual):\n",
    "    geom = PointCloud(cloud, mu.cloud,\n",
    "                      epsilon=sinkhorn_epsilon)\n",
    "    ot_prob = LinearProblem(geom,\n",
    "                            weights,\n",
    "                            mu.weights)\n",
    "    solver = Sinkhorn(**sinkhorn_solver_kwargs)\n",
    "    ot = solver(ot_prob, init=init_dual)\n",
    "    return ot\n",
    "  \n",
    "  if parallel:\n",
    "    if batch_size == -1:\n",
    "        parallel_sinkhorn = jax.vmap(sinkhorn_single_cloud,\n",
    "                                    in_axes=(0, 0, (0, 0)),\n",
    "                                    out_axes=0)\n",
    "        outs = parallel_sinkhorn(*points.unpack(), init_dual)\n",
    "        return outs.g\n",
    "    else:\n",
    "      raise ValueError(\"Not coded yet\") \n",
    "  else:\n",
    "    list_of_g_potentials = []\n",
    "    clouds, weights = points.unpack()\n",
    "    for i in range(len(clouds)):\n",
    "      ot_problem = sinkhorn_single_cloud(clouds[i], weights[i], init_dual)\n",
    "      list_of_g_potentials.append(ot_problem.g)\n",
    "    g_potentials_array = jnp.stack(list_of_g_potentials)\n",
    "    return g_potentials_array\n",
    "  \n",
    "\n",
    "# Set the seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying the new function on toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of distributions\n",
    "num_distributions = 100\n",
    "# Number of sample for each distribution\n",
    "num_sample = 30\n",
    "\n",
    "\n",
    "# Generate random means and variances\n",
    "means = np.random.uniform(low = -0.3, high = 0.3, size = (num_distributions, 2))\n",
    "variances = np.random.uniform(low = 0.0001, high = 0.0004, size = num_distributions)\n",
    "# Generate random samples for each distribution which is the X\n",
    "distributions = [np.random.multivariate_normal(mean, np.eye(2) * variance, num_sample) for mean, variance in zip(means, variances)]\n",
    "\n",
    "\n",
    "# Lets compute the Ys according to the paper's formula\n",
    "y = [(mean[0]+0.5-(mean[1]+0.5)**2)/1+np.sqrt(variance) for mean, variance in zip(means, variances)]\n",
    "\n",
    "## Sampling the mu measure\n",
    "mu_num_sample = 6\n",
    "mu_mean = [0, 0]\n",
    "mu_variance = 0.1\n",
    "mu = np.random.multivariate_normal(mu_mean, np.eye(2) * mu_variance, mu_num_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the list of points mu into a WeightedPointCloud object\n",
    "mu_cloud = WeightedPointCloud(\n",
    "    cloud=jnp.array(mu),\n",
    "    weights=jnp.ones(mu_num_sample)\n",
    ")\n",
    "\n",
    "## First we convert the list all the sampled distributions to WeightedPointCloud objects\n",
    "list_of_weighted_point_clouds = []\n",
    "for sample in distributions:\n",
    "    distrib_cloud = WeightedPointCloud(\n",
    "        cloud=jnp.array(sample),\n",
    "        weights=jnp.ones(len(sample))\n",
    "    )\n",
    "    list_of_weighted_point_clouds.append(distrib_cloud)\n",
    "\n",
    "## We need to convert the cloud list to a VectorizedWeightedPointCloud\n",
    "x_cloud = pad_point_clouds(list_of_weighted_point_clouds)\n",
    "\n",
    "## We choose our epsilon parameter and perform the sinkhirn algorithm\n",
    "sinkhorn_solver_kwargs = {'epsilon': 0.01}\n",
    "sinkhorn_potentials_unchanged = clouds_to_dual_sinkhorn(points = x_cloud, mu = mu_cloud, \n",
    "                                                        sinkhorn_solver_kwargs = sinkhorn_solver_kwargs, \n",
    "                                                        parallel = True, # same as before\n",
    "                                                        batch_size = -1)\n",
    "sinkhorn_solver_kwargs = {'epsilon': 0.01}\n",
    "sinkhorn_potentials_self = clouds_to_dual_sinkhorn(points = x_cloud, mu = mu_cloud, \n",
    "                                                   sinkhorn_solver_kwargs = sinkhorn_solver_kwargs, \n",
    "                                                   parallel = False, # going into the for loop\n",
    "                                                   batch_size = -1)\n",
    "\n",
    "## Our explicative data is now 'sinkhorn_potentials'.\n",
    "## Then we train_test_split our X and Y data\n",
    "x_train_unchanged, x_test_unchanged, y_train_unchanged, y_test_unchanged = train_test_split(sinkhorn_potentials_unchanged, y, test_size = 0.50, random_state = 42)\n",
    "x_train_self, x_test_self, y_train_self, y_test_self = train_test_split(sinkhorn_potentials_self, y, test_size = 0.50, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sinkhorn_potentials_self.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We perform the Kernel Ridge Regression\n",
    "krr = KernelRidge(kernel = \"rbf\")\n",
    "\n",
    "## Define the parameter grid\n",
    "param_grid = {'alpha': [0.01, 0.1, 0.5, 1], 'gamma': [0.01, 0.1, 0.5, 1]}\n",
    "\n",
    "## Create GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator = krr, \n",
    "                           param_grid = param_grid, \n",
    "                           scoring = 'neg_mean_squared_error', \n",
    "                           cv = 5)\n",
    "\n",
    "## Fit the model ie training the model\n",
    "grid_search.fit(X = x_train_unchanged, y = y_train_unchanged)\n",
    "\n",
    "## Get the best parameters from the cross validation\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "## Get the best model\n",
    "my_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "## Obtain predictions for the test set\n",
    "predictions = my_model.predict(X = x_test_unchanged)\n",
    "\n",
    "## Compute the MSE\n",
    "mse = mean_squared_error(y_true = y_test_unchanged, y_pred = predictions)\n",
    "\n",
    "# Compute the EVS\n",
    "evs = explained_variance_score(y_true = y_test_unchanged, y_pred = predictions)\n",
    "\n",
    "# Print the MSE and EVS\n",
    "print(f'Mean Square Error on the Test Set: {mse}')\n",
    "print(f'Explained Variance Score on the Test Set: {evs}')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "sns.set_theme(context='paper', font_scale=1.5)\n",
    "sns.scatterplot(x=y_test_unchanged, y=predictions, color='blue', alpha=0.5)\n",
    "\n",
    "## Adding the x=y line and the text\n",
    "plt.plot([min(y_test_unchanged), max(y_test_unchanged)], [min(y_test_unchanged), max(y_test_unchanged)], linestyle='--', color='red', linewidth=2, alpha = 0.8)\n",
    "plt.text(min(y_test_unchanged), max(predictions), f'EVS = {evs:.3f}', ha='left', va='top', color='black', fontsize=10, weight='bold')\n",
    "\n",
    "plt.title('Predicted vs. Actual Values of Y')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "\n",
    "#plt.savefig('Images/regression_toy_experiment.png', dpi=300, bbox_inches='tight',format=\"png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We perform the Kernel Ridge Regression\n",
    "krr = KernelRidge(kernel = \"rbf\")\n",
    "\n",
    "## Define the parameter grid\n",
    "param_grid = {'alpha': [0.01, 0.1, 0.5, 1], 'gamma': [0.01, 0.1, 0.5, 1]}\n",
    "\n",
    "## Create GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator = krr, \n",
    "                           param_grid = param_grid, \n",
    "                           scoring = 'neg_mean_squared_error', \n",
    "                           cv = 5)\n",
    "\n",
    "## Fit the model ie training the model\n",
    "grid_search.fit(X = x_train_self, y = y_train_self)\n",
    "\n",
    "## Get the best parameters from the cross validation\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "## Get the best model\n",
    "my_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "## Obtain predictions for the test set\n",
    "predictions = my_model.predict(X = x_test_self)\n",
    "\n",
    "## Compute the MSE\n",
    "mse = mean_squared_error(y_true = y_test_self, y_pred = predictions)\n",
    "\n",
    "# Compute the EVS\n",
    "evs = explained_variance_score(y_true = y_test_self, y_pred = predictions)\n",
    "\n",
    "# Print the MSE and EVS\n",
    "print(f'Mean Square Error on the Test Set: {mse}')\n",
    "print(f'Explained Variance Score on the Test Set: {evs}')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "sns.set_theme(context='paper', font_scale=1.5)\n",
    "sns.scatterplot(x=y_test_self, y=predictions, color='blue', alpha=0.5)\n",
    "\n",
    "## Adding the x=y line and the text\n",
    "plt.plot([min(y_test_self), max(y_test_self)], [min(y_test_self), max(y_test_self)], linestyle='--', color='red', linewidth=2, alpha = 0.8)\n",
    "plt.text(min(y_test_self), max(predictions), f'EVS = {evs:.3f}', ha='left', va='top', color='black', fontsize=10, weight='bold')\n",
    "\n",
    "plt.title('Predicted vs. Actual Values of Y')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "\n",
    "#plt.savefig('Images/regression_toy_experiment.png', dpi=300, bbox_inches='tight',format=\"png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be performing the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing on the real dataset\n",
    "\n",
    "Using as single feature the blade's meshes is not convincing. We refer to \"experiment_300_001\" to see the rubbishness.\n",
    "\n",
    "Exploring the Rotor37 dataset is the way to go and incorpore the omega and P values in the input matrix. Then using a kernel on those scalars inputs and the sinkhorn kernel fpr the meshes and than using the product of those two kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the data\n",
    "\n",
    "The cloudpoints and the physics scalars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of blade one want to consider.\n",
    "_many_blades = 50\n",
    "\n",
    "## Creating the list of all file numbers.\n",
    "padded_numbers = [str(i).zfill(9) for i in range(_many_blades)]\n",
    "\n",
    "## Lists that will holds the cloud points and the associated efficiency.\n",
    "distributions = []\n",
    "efficiency = []\n",
    "omega = []\n",
    "P = []\n",
    "\n",
    "for number in padded_numbers:\n",
    "    ## File paths Google Colab\n",
    "    #cgns_file_path = f'/content/drive/MyDrive/Developer/GraduationProject/Experimental Part/Rotor37/dataset/samples/sample_{number}/meshes/mesh_000000000.cgns'\n",
    "    #coefficient_file_path = f'/content/drive/MyDrive/Developer/GraduationProject/Experimental Part/Rotor37/dataset/samples/sample_{number}/scalars.csv'\n",
    "    ## File paths Personal Computer\n",
    "    cgns_file_path = f'Rotor37/dataset/samples/sample_{number}/meshes/mesh_000000000.cgns'\n",
    "    coefficient_file_path = f'Rotor37/dataset/samples/sample_{number}/scalars.csv'\n",
    "    ## Computing the coordinates\n",
    "    x, y, z = read_cgns_coordinates(cgns_file_path)\n",
    "    blade = np.column_stack((x, y, z))\n",
    "    ## Computing the coefficient\n",
    "    scalars = pd.read_csv(coefficient_file_path)\n",
    "    ## Adding to our data\n",
    "    distributions.append(blade)\n",
    "    efficiency.append(scalars[\"Efficiency\"][0])\n",
    "    omega.append(scalars[\"Omega\"][0])\n",
    "    P.append(scalars[\"P\"][0])\n",
    "\n",
    "\n",
    "## Use array objects and reshape them to have the correct form. This is important for the preprocessing steps later.\n",
    "omega = np.array(omega)\n",
    "omega = omega.reshape(-1, 1)\n",
    "\n",
    "P = np.array(P)\n",
    "P = P.reshape(-1, 1)\n",
    "\n",
    "efficiency = np.array(efficiency)\n",
    "efficiency = efficiency.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing Optimal Transport for every blade to the reference measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = distributions[0]\n",
    "\n",
    "## Convert the list of points mu into a WeightedPointCloud object\n",
    "mu_cloud = WeightedPointCloud(\n",
    "    cloud=jnp.array(mu),\n",
    "    weights=jnp.ones(len(mu))\n",
    ")\n",
    "\n",
    "## First we convert the list all the sampled distributions to WeightedPointCloud objects\n",
    "list_of_weighted_point_clouds = []\n",
    "for sample in distributions:\n",
    "    distrib_cloud = WeightedPointCloud(\n",
    "        cloud=jnp.array(sample),\n",
    "        weights=jnp.ones(len(sample))\n",
    "    )\n",
    "    list_of_weighted_point_clouds.append(distrib_cloud)\n",
    "\n",
    "## We need to convert the cloud list to a VectorizedWeightedPointCloud\n",
    "x_cloud = pad_point_clouds(list_of_weighted_point_clouds)\n",
    "\n",
    "## We choose our epsilon parameter and perform the sinkhirn algorithm\n",
    "sinkhorn_solver_kwargs = {'epsilon': 0.1}\n",
    "sinkhorn_potentials = clouds_to_dual_sinkhorn(points = x_cloud, mu = mu_cloud, \n",
    "                                                   sinkhorn_solver_kwargs = sinkhorn_solver_kwargs, \n",
    "                                                   parallel = False, # going into the for loop\n",
    "                                                   batch_size = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing step\n",
    "\n",
    "Here we are going to stack all the features together.\n",
    "- The Sinkhorn potentials that are vectors of size the sample of the reference measure.\n",
    "- The P scalar\n",
    "- The omega scalar\n",
    "\n",
    "Then we are going to compute the kernel matrix by hand, using specific kernels for the Sinkhorn potentials and the additionnals scalars.\n",
    "\n",
    "Therefore, the KernelRidge object must be with \"precomputed\", [see here](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge). Because of that, the parameters of the kernels must be Cross-Validated by hand. We will start by just trying the KRR and Cross Validation on the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the features matrix\n",
    "feature_matrix = np.hstack((sinkhorn_potentials, P, omega))\n",
    "\n",
    "## Define the kernels for different parts of the feature matrix\n",
    "kernel_sinkhorn = kernels.RBF()\n",
    "kernel_scalars = kernels.RBF(length_scale = np.array([1, 1])) # anisotropic kernel\n",
    "\n",
    "## Define the column transformer to apply different kernels to different columns\n",
    "transformers = [\n",
    "    ('blade', FunctionTransformer(None), slice(0, len(mu))),  # Sinkhorn potential goes form 0 to len(reference_measure).\n",
    "    ('scalars', FunctionTransformer(None), slice(len(mu), len(mu)+2)),  # The two scalars are after the sinkhorn potentials.\n",
    "    ('kernel_sinkhorn', kernel_sinkhorn, slice(0, 6)),  # Kernel for the Sinkhorn potentials.\n",
    "    ('kernel_scalars', kernel_scalars, slice(6, 8)),  # Kernel for P and omega.\n",
    "]\n",
    "\n",
    "\n",
    "## Apply the column transformer to the feature matrix. The transformed matrix is now the Kernel Matrix or Gramm Matrix.\n",
    "column_transformer = ColumnTransformer(transformers)\n",
    "kernel_matrix = column_transformer.fit_transform(feature_matrix)\n",
    "\n",
    "\n",
    "## Then we train_test_split our X and Y data\n",
    "x_train, x_test, y_train, y_test = train_test_split(kernel_matrix, efficiency, train_size = 0.7, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We perform the Kernel Ridge Regression\n",
    "krr = KernelRidge(kernel = \"precomputed\")\n",
    "\n",
    "## Define the parameter grid\n",
    "## We can only optimize on the regularization parameter because we use a precomputed kernel.\n",
    "param_grid = {'alpha': [0.001, 0.01, 0.1, 0.5, 1, 10, 100]}\n",
    "\n",
    "## Create GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator = krr, \n",
    "                           param_grid = param_grid, \n",
    "                           scoring = 'neg_mean_squared_error', \n",
    "                           cv = 5)\n",
    "\n",
    "## Fit the model ie training the model\n",
    "grid_search.fit(X = x_train, y = y_train)\n",
    "\n",
    "## Get the best parameters from the cross validation\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "## Get the best model\n",
    "my_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "## Obtain predictions for the test set\n",
    "predictions = my_model.predict(X = x_test)\n",
    "\n",
    "## Compute the MSE\n",
    "mse = mean_squared_error(y_true = y_test, y_pred = predictions)\n",
    "\n",
    "# Compute the EVS\n",
    "evs = explained_variance_score(y_true = y_test, y_pred = predictions)\n",
    "\n",
    "# Print the MSE and EVS\n",
    "print(f'Mean Square Error on the Test Set: {mse}')\n",
    "print(f'Explained Variance Score on the Test Set: {evs}')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "sns.set_theme(context='paper', font_scale=1.5)\n",
    "sns.scatterplot(x=y_test, y=predictions, color='blue', alpha=0.5)\n",
    "\n",
    "## Adding the x=y line and the text\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2, alpha = 0.8)\n",
    "plt.text(min(y_test), max(predictions), f'EVS = {evs:.3f}', ha='left', va='top', color='black', fontsize=10, weight='bold')\n",
    "\n",
    "plt.title('Predicted vs. Actual Values of Y')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "\n",
    "#plt.savefig('Images/regression_toy_experiment.png', dpi=300, bbox_inches='tight',format=\"png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
