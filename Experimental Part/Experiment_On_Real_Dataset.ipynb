{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment on a real dataset\n",
    "\n",
    "This notebook follows the reproduction of the paper's experiment. Here, we apply the Sinkhirn kernel on a real dataset provided [by Safran here](https://plaid-lib.readthedocs.io/en/latest/source/data_challenges/rotor37.html). The data is composed of 3D modelisations of motor blades. Those blades are represented by a cloud points. \n",
    "\n",
    "The goal of this notebook is to perform regression task with X variable being the cloud points of the blade (one cloud point = 1 blade = 1 observation) and the target variable Y is a aerodynamic coefficient provided with the metadata of the blade. \n",
    "- We are going to carry this task using the Sinkhorn kernel of our [paper of interest](https://doi.org/10.48550/arXiv.2210.06574). \n",
    "- After that, we aim at using other kernel to carry out Kernel Ridge Regression, namely the Wassestein kernel and the Sliced Wasserstein kernel, and compare accuracy and computation efficiency of those different methods against the Sinkhorn kernel. \n",
    "- If time allows, the goal is to perform this regression task using Gaussian Processes, again with multiple kernels.\n",
    "\n",
    "<br>\n",
    "\n",
    "Authors of this notebook:\n",
    "* Louis Allain\n",
    "* LÃ©onard Gousset\n",
    "* Julien Heurtin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data\n",
    "In this section we provide the code to import and view the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Needed to import the data\n",
    "import h5py\n",
    "import plotly.graph_objects as go\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define functions to explore the .cgns file\n",
    "def explore_cgns_structure(file_path):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        print(\"File structure:\")\n",
    "        explore_group(f)\n",
    "\n",
    "def explore_group(group, indent=\"\"):\n",
    "    for key in group.keys():\n",
    "        print(f\"{indent}{key}\")\n",
    "        if isinstance(group[key], h5py.Group):\n",
    "            explore_group(group[key], indent + \"  \")\n",
    "\n",
    "\n",
    "## Explore the veyy first blade .cgns file\n",
    "number = '000000001'\n",
    "explore_cgns_structure(f'Rotor37/dataset/samples/sample_{number}/meshes/mesh_000000000.cgns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the coordonates of the cloud points. Lets define a function that can retrieve a cloud points from a .cgns file and the associated aero coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cgns_coordinates(file_path):\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        # We retrieve coordinate by coordinate.\n",
    "        # ! Notice the space before the data. This is due to the naming in the files themselves.\n",
    "        x = np.array(file['Base_2_3/Zone/GridCoordinates/CoordinateX'].get(' data'))\n",
    "        y = np.array(file['Base_2_3/Zone/GridCoordinates/CoordinateY'].get(' data'))\n",
    "        z = np.array(file['Base_2_3/Zone/GridCoordinates/CoordinateZ'].get(' data'))\n",
    "\n",
    "    return x, y, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import any number of blades with their aerodynamic coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of blade one want to consider.\n",
    "_many_blades = 10\n",
    "\n",
    "## Creating the list of all file numbers.\n",
    "padded_numbers = [str(i).zfill(9) for i in range(_many_blades)]\n",
    "\n",
    "## Lists that will holds the cloud points and the associated efficiency.\n",
    "distributions = []\n",
    "efficiency = []\n",
    "P = []\n",
    "omega = []\n",
    "\n",
    "for number in padded_numbers:\n",
    "    ## File paths Google Colab\n",
    "    #cgns_file_path = f'/content/drive/MyDrive/Developer/GraduationProject/Experimental Part/Rotor37/dataset/samples/sample_{number}/meshes/mesh_000000000.cgns'\n",
    "    #coefficient_file_path = f'/content/drive/MyDrive/Developer/GraduationProject/Experimental Part/Rotor37/dataset/samples/sample_{number}/scalars.csv'\n",
    "    ## File paths Personal Computer\n",
    "    cgns_file_path = f'Rotor37/dataset/samples/sample_{number}/meshes/mesh_000000000.cgns'\n",
    "    coefficient_file_path = f'Rotor37/dataset/samples/sample_{number}/scalars.csv'\n",
    "\n",
    "    ## Computing the coordinates\n",
    "    x, y, z = read_cgns_coordinates(cgns_file_path)\n",
    "    blade = np.column_stack((x, y, z))\n",
    "\n",
    "    ## Computing the coefficient\n",
    "    scalars = pd.read_csv(coefficient_file_path)\n",
    "    \n",
    "    ## Adding to our data\n",
    "    distributions.append(blade)\n",
    "    efficiency.append(scalars[\"Efficiency\"][0])\n",
    "    omega.append(scalars[\"Omega\"][0])\n",
    "    P.append(scalars[\"P\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting blades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets consider the very first blade\n",
    "my_blade = distributions[0]\n",
    "\n",
    "# Unpack the blade into separate arrays for x, y and z\n",
    "x, y, z = my_blade[:, 0], my_blade[:, 1], my_blade[:, 2]\n",
    "\n",
    "plt.figure()\n",
    "# Create a DataFrame for Seaborn (not necessary for this example)\n",
    "data = {'x': x, 'y': y, 'z': z}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create 3D scatter plot with Matplotlib\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot\n",
    "ax.scatter(x, y, z, s=50, c='blue', marker='o')\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('X-axis')\n",
    "ax.set_ylabel('Y-axis')\n",
    "ax.set_zlabel('Z-axis')\n",
    "\n",
    "# Save the figure\n",
    "#plt.savefig(\"Images/blade_static_plot.png\", dpi=300, bbox_inches='tight', format=\"png\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets consider the very first blade\n",
    "my_blade = distributions[0]\n",
    "\n",
    "# Unpack the blade into separate arrays for x, y and z\n",
    "x, y, z = my_blade[:, 0], my_blade[:, 1], my_blade[:, 2]\n",
    "\n",
    "plt.figure()\n",
    "# Create 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z, mode='markers', marker=dict(size=8, color='blue'))])\n",
    "\n",
    "# Update layout for better visualization\n",
    "fig.update_layout(scene=dict(aspectmode='data'))\n",
    "\n",
    "# Set axis labels\n",
    "fig.update_layout(scene=dict(xaxis_title='X-axis', yaxis_title='Y-axis', zaxis_title='Z-axis'))\n",
    "\n",
    "# To export the plot\n",
    "#fig.write_html(\"blade_plotly_figure.html\")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Sinkhorn kernel to perform Kernel Ridge Regression\n",
    "\n",
    "The **first** problem that arises is the computation time. Each blade is made of about $30,000$ points. Performing Regularized Optimal Transport on such a big distribution's sample is long. Moreover, the function used in the toy experiment previoulsy uses the ```clouds_to_dual_sinkhorn``` function to perform ROT between two cloud points. This function uses a ```jax.vmap``` to perform the computation. Unfortunately this function has a very high usage of memory (which is needed to perform the computations in parallel) and is therefore not viable for our problem's size. Multiple solutions to bypass this issue:\n",
    "1) Undersample every blade. Instead of a blade being $30,000$, we randomly select a portion of those points.\n",
    "2) Undersample the training set. Instead of using the full $1,200$ blades we consider only a fraction of that.\n",
    "3) Changing the way the computation is done. ```jax.vmap``` is very time effective. We could sacrifice time for memory, for instance by computing the ROT sequentially rather than parallely.\n",
    "\n",
    "The **second** question is *which reference measure to consider ?* Again multiple ideas come to mind:\n",
    "1) Using the first blade as the reference measure.\n",
    "2) Taking the smallest rectangle in which all the blades fit and sampling points on its borders.\n",
    "3) Taking random points in approximately a good range?\n",
    "\n",
    "Many questions are still un-answered:\n",
    "1) How many points should make the reference measure ?\n",
    "2) For points 3., what should be the variance of the distribution ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Optimal Transport between two blades\n",
    "\n",
    "This section is dedicated to perform ROT between two cloud points. It is mainly here to help us understand every steps needed to perform ROT using ```jax``` and ```ott``` packages. From here we will determine the best way to carry the ROT problem between two large samples, trading off between memory and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jax package allows to speed up computation\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Packages that actually performs Sinkhorn algorithm\n",
    "from ott.geometry.pointcloud import PointCloud\n",
    "from ott.problems.linear.linear_problem import LinearProblem\n",
    "from ott.solvers.linear.sinkhorn import Sinkhorn\n",
    "import ott\n",
    "\n",
    "\n",
    "# Start a timer\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "## First les take two first blade\n",
    "blade_one = distributions[1]\n",
    "blade_two = distributions[2]\n",
    "\n",
    "## How many points per blade ?\n",
    "size_cloud_points = len(blade_one)\n",
    "\n",
    "## We need to define the clouds as jax.numpy arrays\n",
    "cloud_one = jnp.array(blade_one)\n",
    "cloud_two = jnp.array(blade_two)\n",
    "\n",
    "## Define the epsilon for Regularized OT - small epsilon = great accuracy = longer time\n",
    "epsilon = 1\n",
    "\n",
    "## Create PointCloud object to accomodate OTT package\n",
    "my_geom = PointCloud(x = cloud_one, # training cloud or first blade cloud\n",
    "                  y = cloud_two, # reference cloud or second blade cloud\n",
    "                  epsilon = epsilon # epsilon of ROT\n",
    "                  )\n",
    "\n",
    "## Formalises the Regularized Optimal Transport problem\n",
    "ot_problem = LinearProblem(geom = my_geom) # The geometry of the problem between the two blades\n",
    "\n",
    "## Instanciate the solver of the ROT problem\n",
    "sinkhorn_solver =  Sinkhorn()\n",
    "\n",
    "## Actually computing the Sinkhorn algortihm\n",
    "rot_result = sinkhorn_solver(ot_problem)\n",
    "\n",
    "# Stops the timer\n",
    "end_time = time.time()\n",
    "time_spent = end_time - start_time\n",
    "print(\"Regularized Optimal Transport computation took:\", time_spent, \"seconds, using epsilon =\", epsilon)\n",
    "\n",
    "\n",
    "## Retrieve left and right potentials\n",
    "left_potentials = rot_result.f\n",
    "right_potentials = rot_result.g\n",
    "\n",
    "## Retrieve the duals potentials\n",
    "dual_potentials = rot_result.to_dual_potentials()\n",
    "\n",
    "## Compute the transport plan matrix\n",
    "transport_plan_matrix = rot_result.matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Transport Plan Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Heatmap visualization of transport plan matrix\n",
    "plt.imshow(transport_plan_matrix, cmap = \"Purples\")\n",
    "plt.title('Transport Plan Heatmap')\n",
    "plt.colorbar()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"Images/transport_plan_matrix_2_blades.png\", dpi=300, bbox_inches='tight', format=\"png\")\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Sinkhorn potentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for left and right potentials\n",
    "left_color = 'blue'\n",
    "right_color = 'orange'\n",
    "\n",
    "# Calculate the difference between left and right potentials\n",
    "potential_difference = [left - right for left, right in zip(left_potentials, right_potentials)]\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(18, 4))  # Adjust figsize as needed\n",
    "\n",
    "# Left potentials plot\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.plot(left_potentials, label='Left Potentials', color=left_color)\n",
    "plt.legend()\n",
    "plt.title('Left Potentials')\n",
    "\n",
    "# Dual potentials plot\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.plot(left_potentials, label='Left Potentials', color=left_color)\n",
    "plt.plot(right_potentials, label='Right Potentials', color=right_color)\n",
    "plt.legend()\n",
    "plt.title('Dual Potentials')\n",
    "\n",
    "# Right potentials plot\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.plot(right_potentials, label='Right Potentials', color=right_color)\n",
    "plt.legend()\n",
    "plt.title('Right Potentials')\n",
    "\n",
    "# Potential difference plot\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.plot(potential_difference, label='Potential Difference', color='green')\n",
    "plt.legend()\n",
    "plt.title('Potential Difference')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig(\"Images/transport_plan_matrix_2_blades.png\", dpi=300, bbox_inches='tight', format=\"png\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the transport map\n",
    "\n",
    "Using the tutorial [here](https://ott-jax.readthedocs.io/en/latest/tutorials/point_clouds.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This does not work, but I cannot figure out why.\n",
    "\n",
    "# Plotting utility\n",
    "def plot_map(x, y, z, forward: bool = True):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    marker_t = \"o\" if forward else \"X\"\n",
    "    label = (\n",
    "        r\"$T_{x\\rightarrow y}(x)$\" if forward else r\"$T_{y\\rightarrow x}(y)$\"\n",
    "    )\n",
    "    w = x if forward else y\n",
    "    plt.quiver(\n",
    "        *w.T,\n",
    "        *(z - w).T,\n",
    "        color=\"k\",\n",
    "        angles=\"xy\",\n",
    "        scale_units=\"xy\",\n",
    "        scale=1,\n",
    "        width=0.007,\n",
    "    )\n",
    "    plt.scatter(*x.T, s=200, edgecolors=\"k\", marker=\"o\", label=r\"$x$\")\n",
    "    plt.scatter(*y.T, s=200, edgecolors=\"k\", marker=\"X\", label=r\"$y$\")\n",
    "    plt.scatter(*z.T, s=150, edgecolors=\"k\", marker=marker_t, label=label)\n",
    "    plt.legend(fontsize=22)\n",
    "\n",
    "# Computing the transport\n",
    "transport = dual_potentials.transport(cloud_one)\n",
    "\n",
    "# Actually plotting the transpot map\n",
    "plot_map(cloud_one, cloud_two, transport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some other way, that seems to work but the plot is not usable/readable...\n",
    "\n",
    "plott = ott.tools.plot.Plot()\n",
    "_ = plott(rot_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Optimal Transport between a reference measure and a blade\n",
    "\n",
    "The reference measure considered here is a sphere of center the barycenter of the blade and of small radius (0.04). This refrence measure is only considered to perform Regularized Optimal Transport and to analyse its behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a few utilities functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_barycenter(points):\n",
    "    num_points = len(points)\n",
    "    if num_points == 0:\n",
    "        raise ValueError(\"Cannot calculate the barycenter of an empty set of points.\")\n",
    "    \n",
    "    # Sum the coordinates along each axis\n",
    "    sum_x = np.sum(points[:, 0])\n",
    "    sum_y = np.sum(points[:, 1])\n",
    "    sum_z = np.sum(points[:, 2])\n",
    "    \n",
    "    # Calculate the barycenter coordinates\n",
    "    barycenter_x = sum_x / num_points\n",
    "    barycenter_y = sum_y / num_points\n",
    "    barycenter_z = sum_z / num_points\n",
    "    \n",
    "    return barycenter_x, barycenter_y, barycenter_z\n",
    "\n",
    "def sample_points_on_sphere(num_points, radius = 1, center = (0, 0, 0)):\n",
    "    # Generate random values for Î¸ and Ï\n",
    "    theta = np.random.uniform(0, np.pi, num_points)\n",
    "    phi = np.random.uniform(0, 2*np.pi, num_points)\n",
    "    \n",
    "    # Calculate Cartesian coordinates\n",
    "    x = center[0] + radius * np.sin(theta) * np.cos(phi)\n",
    "    y = center[1] + radius * np.sin(theta) * np.sin(phi)\n",
    "    z = center[2] + radius * np.cos(theta)\n",
    "    \n",
    "    return x, y, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the blade and sampling the reference measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choosing the blade\n",
    "blade_one = distributions[1]\n",
    "x, y, z = blade_one[:, 0], blade_one[:, 1], blade_one[:, 2]\n",
    "# Calculate the barycenter of the blade\n",
    "barycenter_x, barycenter_y, barycenter_z = calculate_barycenter(blade_one)\n",
    "\n",
    "## Sampling the sphere of a given radius and of center the barycenter of the blade\n",
    "num_points = 1000\n",
    "radius = 0.04\n",
    "ref_x, ref_y, ref_z = sample_points_on_sphere(num_points, radius = radius, center = (barycenter_x, barycenter_y, barycenter_z))\n",
    "reference_measure_sample = np.column_stack((ref_x, ref_y, ref_z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the blade and the reference measure sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting plotly figure\n",
    "\n",
    "# Create traces for each set of coordinates\n",
    "trace1 = go.Scatter3d(x=ref_x, y=ref_y, z=ref_z, mode='markers', marker=dict(size=8, color='blue'), name='Reference Measure')\n",
    "trace2 = go.Scatter3d(x=x, y=y, z=z, mode='markers', marker=dict(size=8, color='red'), name='Blade')\n",
    "\n",
    "# Create the figure with both traces\n",
    "fig = go.Figure(data=[trace1, trace2])\n",
    "\n",
    "# Update layout for better visualization\n",
    "fig.update_layout(scene=dict(aspectmode='data'))\n",
    "\n",
    "# Set axis labels\n",
    "fig.update_layout(scene=dict(xaxis_title='X-axis', yaxis_title='Y-axis', zaxis_title='Z-axis'))\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Regularized Optimal Transport between the blade and the reference measure sample\n",
    "\n",
    "The functions from the paper normalized the reference measure. Is it necessary here ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jax package allows to speed up computation\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Packages that actually performs Sinkhorn algorithm\n",
    "from ott.geometry.pointcloud import PointCloud\n",
    "from ott.problems.linear.linear_problem import LinearProblem\n",
    "from ott.solvers.linear.sinkhorn import Sinkhorn\n",
    "import ott\n",
    "\n",
    "## We sample from our reference measure.\n",
    "\n",
    "# Start a timer\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "## Our cloud points are in the variables blade_one and reference_measure_sample\n",
    "\n",
    "## How many points per blade ?\n",
    "size_cloud_points = len(blade_one)\n",
    "\n",
    "## We need to define the clouds as jax.numpy arrays\n",
    "cloud_one = jnp.array(blade_one)\n",
    "cloud_ref = jnp.array(reference_measure_sample)\n",
    "\n",
    "## The functions from the paper normalized the reference measure. Is it necessary here ?\n",
    "\n",
    "## Define the epsilon for Regularized OT - small epsilon = great accuracy = longer time\n",
    "epsilon = 1\n",
    "\n",
    "## Create PointCloud object to accomodate OTT package\n",
    "my_geom = PointCloud(x = cloud_one, # training cloud or first blade cloud\n",
    "                  y = cloud_ref, # reference cloud or second blade cloud\n",
    "                  epsilon = epsilon # epsilon of ROT\n",
    "                  )\n",
    "\n",
    "## Formalises the Regularized Optimal Transport problem\n",
    "ot_problem = LinearProblem(geom = my_geom) # The geometry of the problem between the two blades\n",
    "\n",
    "## Instanciate the solver of the ROT problem\n",
    "sinkhorn_solver =  Sinkhorn()\n",
    "\n",
    "## Actually computing the Sinkhorn algortihm\n",
    "rot_result = sinkhorn_solver(ot_problem)\n",
    "\n",
    "# Stops the timer\n",
    "end_time = time.time()\n",
    "time_spent = end_time - start_time\n",
    "print(\"Regularized Optimal Transport computation took:\", time_spent, \"seconds, using epsilon =\", epsilon)\n",
    "\n",
    "## Retrieve left and right potentials\n",
    "left_potentials = rot_result.f\n",
    "right_potentials = rot_result.g\n",
    "\n",
    "## Retrieve the duals potentials\n",
    "dual_potentials = rot_result.to_dual_potentials()\n",
    "\n",
    "## Compute the transport plan matrix\n",
    "transport_plan_matrix = rot_result.matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the Kernel Ridge Regression from scratch\n",
    "\n",
    "The functions from the paper normalized the reference measure. Is it necessary here ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_barycenter(points_list):\n",
    "    total_points = 0\n",
    "    sum_x = 0\n",
    "    sum_y = 0\n",
    "    sum_z = 0\n",
    "    \n",
    "    # Iterate over each set of points\n",
    "    for points in points_list:\n",
    "        num_points = len(points)\n",
    "        if num_points == 0:\n",
    "            raise ValueError(\"Cannot calculate the barycenter of an empty set of points.\")\n",
    "        \n",
    "        # Sum the coordinates along each axis for this set of points\n",
    "        sum_x += np.sum(points[:, 0])\n",
    "        sum_y += np.sum(points[:, 1])\n",
    "        sum_z += np.sum(points[:, 2])\n",
    "        \n",
    "        total_points += num_points\n",
    "    \n",
    "    # Calculate the global barycenter coordinates\n",
    "    barycenter_x = sum_x / total_points\n",
    "    barycenter_y = sum_y / total_points\n",
    "    barycenter_z = sum_z / total_points\n",
    "    \n",
    "    return barycenter_x, barycenter_y, barycenter_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data and train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "\n",
    "## Number of blade one want to consider.\n",
    "_many_blades = 10\n",
    "\n",
    "## Creating the list of all file numbers.\n",
    "padded_numbers = [str(i).zfill(9) for i in range(_many_blades)]\n",
    "\n",
    "## Lists that will holds the cloud points and the associated efficiency.\n",
    "distributions = []\n",
    "efficiency = []\n",
    "\n",
    "for number in padded_numbers:\n",
    "    ## File paths Google Colab\n",
    "    #cgns_file_path = f'/content/drive/MyDrive/Developer/GraduationProject/Experimental Part/Rotor37/dataset/samples/sample_{number}/meshes/mesh_000000000.cgns'\n",
    "    #coefficient_file_path = f'/content/drive/MyDrive/Developer/GraduationProject/Experimental Part/Rotor37/dataset/samples/sample_{number}/scalars.csv'\n",
    "    ## File paths Personal Computer\n",
    "    cgns_file_path = f'Rotor37/dataset/samples/sample_{number}/meshes/mesh_000000000.cgns'\n",
    "    coefficient_file_path = f'Rotor37/dataset/samples/sample_{number}/scalars.csv'\n",
    "    ## Computing the coordinates\n",
    "    x, y, z = read_cgns_coordinates(cgns_file_path)\n",
    "    blade = np.column_stack((x, y, z))\n",
    "    ## Computing the coefficient\n",
    "    scalars = pd.read_csv(coefficient_file_path)\n",
    "    ## Adding to our data\n",
    "    distributions.append(blade)\n",
    "    efficiency.append(scalars[\"Efficiency\"][0])\n",
    "\n",
    "## Train Test split of 70%\n",
    "x_train, x_test, y_train, y_test = train_test_split(distributions, efficiency, train_size = 0.7, random_state = 42)\n",
    "\n",
    "## Transforming the train data to jnp.array\n",
    "clouds_train = jnp.array(x_train)\n",
    "clouds_test = jnp.array(x_test)\n",
    "\n",
    "## Computing the barycentre of all clouds\n",
    "x_barycenter, y_barycenter, z_barycenter = global_barycenter(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the reference measure which depends on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sampling the sphere of a given radius and of center the barycenter of the blade\n",
    "num_points = 1000\n",
    "radius = 0.04\n",
    "\n",
    "## Sampling the reference measure. Note that the barycenter depends of the training data\n",
    "ref_x, ref_y, ref_z = sample_points_on_sphere(num_points, radius = radius, center = (x_barycenter, y_barycenter, z_barycenter))\n",
    "reference_measure_sample = np.column_stack((ref_x, ref_y, ref_z))\n",
    "\n",
    "## Using jnp.array objects\n",
    "cloud_ref = jnp.array(reference_measure_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the potentials for each ROT problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_potentials = []\n",
    "\n",
    "## Define the epsilon for Regularized OT - small epsilon = great accuracy = longer time\n",
    "epsilon = 1\n",
    "\n",
    "for cloud in clouds_train:\n",
    "\n",
    "    ## Create PointCloud object to accomodate OTT package\n",
    "    my_geom = PointCloud(x = cloud, # training cloud\n",
    "                         y = cloud_ref, # reference cloud\n",
    "                         epsilon = epsilon # epsilon of ROT\n",
    "                         )\n",
    "\n",
    "    ## Formalises the Regularized Optimal Transport problem\n",
    "    ot_problem = LinearProblem(geom = my_geom) # The geometry of the problem between the two blades\n",
    "\n",
    "    ## Instanciate the solver of the ROT problem\n",
    "    sinkhorn_solver =  Sinkhorn()\n",
    "\n",
    "    ## Actually computing the Sinkhorn algortihm\n",
    "    rot_result = sinkhorn_solver(ot_problem)\n",
    "    \n",
    "    ## Retrieve RIGHT potentials (we are only interested in those)\n",
    "    right_potentials = rot_result.g\n",
    "\n",
    "    x_train_potentials.append(right_potentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We perform the Kernel Ridge Regression\n",
    "krr = KernelRidge(kernel = \"rbf\")\n",
    "\n",
    "## Define the parameter grid\n",
    "param_grid = {'alpha': [0.01, 0.1, 1, 10], 'gamma': [0.01, 0.1, 1, 10]}\n",
    "\n",
    "## Create GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator = krr, \n",
    "                           param_grid = param_grid, \n",
    "                           scoring = 'neg_mean_squared_error', \n",
    "                           cv = 5)\n",
    "\n",
    "## Fit the model ie training the model\n",
    "grid_search.fit(X = x_train_potentials, y = y_train)\n",
    "\n",
    "## Get the best parameters from the cross validation\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "## Get the best model\n",
    "my_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing ROT on test data and testing the Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_potentials = []\n",
    "\n",
    "## Define the epsilon for Regularized OT - small epsilon = great accuracy = longer time\n",
    "epsilon = 1\n",
    "\n",
    "for cloud in clouds_test:\n",
    "\n",
    "    ## Create PointCloud object to accomodate OTT package\n",
    "    my_geom = PointCloud(x = cloud, # training cloud\n",
    "                         y = cloud_ref, # reference cloud\n",
    "                         epsilon = epsilon # epsilon of ROT\n",
    "                         )\n",
    "\n",
    "    ## Formalises the Regularized Optimal Transport problem\n",
    "    ot_problem = LinearProblem(geom = my_geom) # The geometry of the problem between the two blades\n",
    "\n",
    "    ## Instanciate the solver of the ROT problem\n",
    "    sinkhorn_solver =  Sinkhorn()\n",
    "\n",
    "    ## Actually computing the Sinkhorn algortihm\n",
    "    rot_result = sinkhorn_solver(ot_problem)\n",
    "    \n",
    "    ## Retrieve RIGHT potentials (we are only interested in those)\n",
    "    right_potentials = rot_result.g\n",
    "\n",
    "    x_test_potentials.append(right_potentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtain predictions for the test set\n",
    "predictions = my_model.predict(X = x_test_potentials)\n",
    "\n",
    "## Compute the MSE\n",
    "mse = mean_squared_error(y_true = y_test, y_pred = predictions)\n",
    "\n",
    "# Compute the EVS\n",
    "evs = explained_variance_score(y_true = y_test, y_pred = predictions)\n",
    "\n",
    "# Print the MSE and EVS\n",
    "print(f'Mean Square Error on the Test Set: {mse}')\n",
    "print(f'Explained Variance Score on the Test Set: {evs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "sns.set_theme(context='paper', font_scale=1.5)\n",
    "sns.scatterplot(x=y_test, y=predictions, color='blue', alpha=0.5)\n",
    "\n",
    "## Adding the x=y line and the text\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2, alpha = 0.8)\n",
    "plt.text(min(y_test), max(predictions), f'EVS = {evs:.3f}', ha='left', va='top', color='black', fontsize=10, weight='bold')\n",
    "\n",
    "plt.title('Predicted vs. Actual Values of Y')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "\n",
    "plt.savefig('Images/regression_500_blades.png', dpi=300, bbox_inches='tight',format=\"png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the Kernel Ridge Regression with the paper's functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nx/gcjsd2hn49l_4ypjlnw3m0d00000gn/T/ipykernel_1563/1537333419.py:23: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from dataclasses import replace\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import struct\n",
    "import optax as ox\n",
    "\n",
    "# Packages that actually performs Sinkhorn algorithm\n",
    "from ott.geometry.pointcloud import PointCloud\n",
    "from ott.problems.linear.linear_problem import LinearProblem\n",
    "from ott.solvers.linear.sinkhorn import Sinkhorn\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.gaussian_process import kernels\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import h5py\n",
    "import plotly.graph_objects as go\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def read_cgns_coordinates(file_path):\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        # We retrieve coordinate by coordinate.\n",
    "        # ! Notice the space before the data. This is due to the naming in the files themselves.\n",
    "        x = np.array(file['Base_2_3/Zone/GridCoordinates/CoordinateX'].get(' data'))\n",
    "        y = np.array(file['Base_2_3/Zone/GridCoordinates/CoordinateY'].get(' data'))\n",
    "        z = np.array(file['Base_2_3/Zone/GridCoordinates/CoordinateZ'].get(' data'))\n",
    "\n",
    "    return x, y, z\n",
    "\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class WeightedPointCloud:\n",
    "  \"\"\"A weighted point cloud.\n",
    "  \n",
    "  Attributes:\n",
    "    cloud: Array of shape (n, d) where n is the number of points and d the dimension.\n",
    "    weights: Array of shape (n,) where n is the number of points.\n",
    "  \"\"\"\n",
    "  cloud: jnp.array\n",
    "  weights: jnp.array\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.cloud.shape[0]\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class VectorizedWeightedPointCloud:\n",
    "  \"\"\"Vectorized version of WeightedPointCloud.\n",
    "\n",
    "  Assume that b clouds are all of size n and dimension d.\n",
    "  \n",
    "  Attributes:\n",
    "    _private_cloud: Array of shape (b, n, d) where n is the number of points and d the dimension.\n",
    "    _private_weights: Array of shape (b, n) where n is the number of points.\n",
    "  \n",
    "  Methods:\n",
    "    unpack: returns the cloud and weights.\n",
    "  \"\"\"\n",
    "  _private_cloud: jnp.array\n",
    "  _private_weights: jnp.array\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return WeightedPointCloud(self._private_cloud[idx], self._private_cloud[idx])\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self._private_cloud.shape[0]\n",
    "  \n",
    "  def __iter__(self):\n",
    "    for i in range(len(self)):\n",
    "      yield self[i]\n",
    "\n",
    "  def unpack(self):\n",
    "    return self._private_cloud, self._private_weights\n",
    "\n",
    "def pad_point_cloud(point_cloud, max_cloud_size, fail_on_too_big=True):\n",
    "  \"\"\"Pad a single point cloud with zeros to have the same size.\n",
    "  \n",
    "  Args:\n",
    "    point_cloud: a weighted point cloud.\n",
    "    max_cloud_size: the size of the biggest point cloud.\n",
    "    fail_on_too_big: if True, raise an error if the cloud is too big for padding.\n",
    "  \n",
    "  Returns:\n",
    "    a WeightedPointCloud with padded cloud and weights.\n",
    "  \"\"\"\n",
    "  cloud, weights = point_cloud.cloud, point_cloud.weights\n",
    "  delta = max_cloud_size - cloud.shape[0]\n",
    "  if delta <= 0:\n",
    "    if fail_on_too_big:\n",
    "      assert False, 'Cloud is too big for padding.'\n",
    "    return point_cloud\n",
    "\n",
    "  ratio = 1e-3  # less than 0.1% of the total mass.\n",
    "  smallest_weight = jnp.min(weights) / delta * ratio\n",
    "  small_weights = jnp.ones(delta) * smallest_weight\n",
    "\n",
    "  weights = weights * (1 - ratio)  # keep 99.9% of the mass.\n",
    "  weights = jnp.concatenate([weights, small_weights], axis=0)\n",
    "\n",
    "  cloud = jnp.pad(cloud, pad_width=((0, delta), (0,0)), mode='mean')\n",
    "\n",
    "  point_cloud = WeightedPointCloud(cloud, weights)\n",
    "\n",
    "  return point_cloud\n",
    "\n",
    "def pad_point_clouds(cloud_list):\n",
    "  \"\"\"Pad the point clouds with zeros to have the same size.\n",
    "\n",
    "  Note: this function should be used outside of jax.jit because the computation graph\n",
    "        is huge. O(len(cloud_list)) nodes are generated.\n",
    "\n",
    "  Args:\n",
    "    cloud_list: a list of WeightedPointCloud.\n",
    "  \n",
    "  Returns:\n",
    "    a VectrorizedWeightedPointCloud with padded clouds and weights.\n",
    "  \"\"\"\n",
    "  # sentinel for unified processing of all clouds, including biggest one.\n",
    "  max_cloud_size = max([len(cloud) for cloud in cloud_list]) + 1\n",
    "  sentinel_padder = partial(pad_point_cloud, max_cloud_size=max_cloud_size)\n",
    "\n",
    "  cloud_list = list(map(sentinel_padder, cloud_list))\n",
    "  coordinates = jnp.stack([cloud.cloud for cloud in cloud_list])\n",
    "  weights = jnp.stack([cloud.weights for cloud in cloud_list])\n",
    "  return VectorizedWeightedPointCloud(coordinates, weights)\n",
    "\n",
    "def clouds_barycenter(points):\n",
    "  \"\"\"Compute the barycenter of a set of clouds.\n",
    "  \n",
    "  Args:\n",
    "    points: a VectorizedWeightedPointCloud.\n",
    "    \n",
    "  Returns:\n",
    "    a barycenter of the clouds of points, of shape (1, d) where d is the dimension.\n",
    "  \"\"\"\n",
    "  clouds, weights = points.unpack()\n",
    "  barycenter = jnp.sum(clouds * weights[:,:,jnp.newaxis], axis=1)\n",
    "  barycenter = jnp.mean(barycenter, axis=0, keepdims=True)\n",
    "  return barycenter\n",
    "\n",
    "\n",
    "def to_simplex(mu):\n",
    "  \"\"\"Project weights to the simplex.\n",
    "  \n",
    "  Args: \n",
    "    mu: a WeightedPointCloud.\n",
    "    \n",
    "  Returns:\n",
    "    a WeightedPointCloud with weights projected to the simplex.\"\"\"\n",
    "  if mu.weights is None:\n",
    "    mu_weights = None\n",
    "  else:\n",
    "    mu_weights = jax.nn.softmax(mu.weights)\n",
    "  return replace(mu, weights=mu_weights)\n",
    "\n",
    "\n",
    "def reparametrize_mu(mu, cloud_barycenter, scale):\n",
    "  \"\"\"Re-parametrize mu to be invariant by translation and scaling.\n",
    "\n",
    "  Args:\n",
    "    mu: a WeightedPointCloud.\n",
    "    cloud_barycenter: Array of shape (1, d) where d is the dimension.\n",
    "    scale: float, scaling parameter for the re-parametrization of mu.\n",
    "  \n",
    "  Returns:\n",
    "    a WeightedPointCloud with re-parametrized weights and cloud.\n",
    "  \"\"\"\n",
    "  # invariance by translation : recenter mu around its mean\n",
    "  mu_cloud = mu.cloud - jnp.mean(mu.cloud, axis=0, keepdims=True)  # center.\n",
    "  mu_cloud = scale * jnp.tanh(mu_cloud)  # re-parametrization of the domain.\n",
    "  mu_cloud = mu_cloud + cloud_barycenter  # re-center toward barycenter of all clouds.\n",
    "  return replace(mu, cloud=mu_cloud)\n",
    "\n",
    "\n",
    "def clouds_to_dual_sinkhorn(points, \n",
    "                            mu, \n",
    "                            init_dual=(None, None),\n",
    "                            scale=1.,\n",
    "                            has_aux=False,\n",
    "                            sinkhorn_solver_kwargs=None, \n",
    "                            parallel: bool = True,\n",
    "                            batch_size: int = -1):\n",
    "  \"\"\"Compute the embeddings of the clouds with regularized OT towards mu.\n",
    "  \n",
    "  Args:\n",
    "    points: a VectorizedWeightedPointCloud.\n",
    "    init_dual: tuple of two arrays of shape (b, n) and (b, m) where b is the number of clouds,\n",
    "               n is the number of points in each cloud, and m the number of points in mu.\n",
    "    scale: float, scaling parameter for the re-parametrization of mu.\n",
    "    has_aux: bool, whether to return the full Sinkhorn output or only the dual variables.\n",
    "    sinkhorn_solver_kwargs: dict, kwargs for the Sinkhorn solver.\n",
    "      Must contain the key 'epsilon' for the regularization parameter.\n",
    "\n",
    "  Returns:\n",
    "    a tuple (dual, init_dual) with dual variables of shape (n, m) where n is the number of points\n",
    "    and m the number of points in mu, and init_dual a tuple (init_dual_cloud, init_dual_mu) \n",
    "  \"\"\"\n",
    "  sinkhorn_epsilon = sinkhorn_solver_kwargs.pop('epsilon')\n",
    "  \n",
    "  # weight projection\n",
    "  barycenter = clouds_barycenter(points)\n",
    "  mu = to_simplex(mu)\n",
    "\n",
    "  # cloud projection\n",
    "  mu = reparametrize_mu(mu, barycenter, scale)\n",
    "\n",
    "  def sinkhorn_single_cloud(cloud, weights, init_dual):\n",
    "    geom = PointCloud(cloud, mu.cloud,\n",
    "                      epsilon=sinkhorn_epsilon)\n",
    "    ot_prob = LinearProblem(geom,\n",
    "                            weights,\n",
    "                            mu.weights)\n",
    "    solver = Sinkhorn(**sinkhorn_solver_kwargs)\n",
    "    ot = solver(ot_prob, init=init_dual)\n",
    "    return ot\n",
    "  \n",
    "  if parallel:\n",
    "    if batch_size == -1:\n",
    "        parallel_sinkhorn = jax.vmap(sinkhorn_single_cloud,\n",
    "                                    in_axes=(0, 0, (0, 0)),\n",
    "                                    out_axes=0)\n",
    "        outs = parallel_sinkhorn(*points.unpack(), init_dual)\n",
    "        return outs.g\n",
    "    else:\n",
    "      raise ValueError(\"Not coded yet\") \n",
    "  else:\n",
    "    list_of_g_potentials = []\n",
    "    clouds, weights = points.unpack()\n",
    "    for i in range(len(clouds)):\n",
    "      ot_problem = sinkhorn_single_cloud(clouds[i], weights[i], init_dual)\n",
    "      list_of_g_potentials.append(ot_problem.g)\n",
    "    g_potentials_array = jnp.stack(list_of_g_potentials)\n",
    "    return g_potentials_array\n",
    "  \n",
    "\n",
    "# Set the seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying the new function on toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of distributions\n",
    "num_distributions = 100\n",
    "# Number of sample for each distribution\n",
    "num_sample = 30\n",
    "\n",
    "\n",
    "# Generate random means and variances\n",
    "means = np.random.uniform(low = -0.3, high = 0.3, size = (num_distributions, 2))\n",
    "variances = np.random.uniform(low = 0.0001, high = 0.0004, size = num_distributions)\n",
    "# Generate random samples for each distribution which is the X\n",
    "distributions = [np.random.multivariate_normal(mean, np.eye(2) * variance, num_sample) for mean, variance in zip(means, variances)]\n",
    "\n",
    "\n",
    "# Lets compute the Ys according to the paper's formula\n",
    "y = [(mean[0]+0.5-(mean[1]+0.5)**2)/1+np.sqrt(variance) for mean, variance in zip(means, variances)]\n",
    "\n",
    "## Sampling the mu measure\n",
    "mu_num_sample = 6\n",
    "mu_mean = [0, 0]\n",
    "mu_variance = 0.1\n",
    "mu = np.random.multivariate_normal(mu_mean, np.eye(2) * mu_variance, mu_num_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the list of points mu into a WeightedPointCloud object\n",
    "mu_cloud = WeightedPointCloud(\n",
    "    cloud=jnp.array(mu),\n",
    "    weights=jnp.ones(mu_num_sample)\n",
    ")\n",
    "\n",
    "## First we convert the list all the sampled distributions to WeightedPointCloud objects\n",
    "list_of_weighted_point_clouds = []\n",
    "for sample in distributions:\n",
    "    distrib_cloud = WeightedPointCloud(\n",
    "        cloud=jnp.array(sample),\n",
    "        weights=jnp.ones(len(sample))\n",
    "    )\n",
    "    list_of_weighted_point_clouds.append(distrib_cloud)\n",
    "\n",
    "## We need to convert the cloud list to a VectorizedWeightedPointCloud\n",
    "x_cloud = pad_point_clouds(list_of_weighted_point_clouds)\n",
    "\n",
    "## We choose our epsilon parameter and perform the sinkhirn algorithm\n",
    "sinkhorn_solver_kwargs = {'epsilon': 0.01}\n",
    "sinkhorn_potentials_unchanged = clouds_to_dual_sinkhorn(points = x_cloud, mu = mu_cloud, \n",
    "                                                        sinkhorn_solver_kwargs = sinkhorn_solver_kwargs, \n",
    "                                                        parallel = True, # same as before\n",
    "                                                        batch_size = -1)\n",
    "sinkhorn_solver_kwargs = {'epsilon': 0.01}\n",
    "sinkhorn_potentials_self = clouds_to_dual_sinkhorn(points = x_cloud, mu = mu_cloud, \n",
    "                                                   sinkhorn_solver_kwargs = sinkhorn_solver_kwargs, \n",
    "                                                   parallel = False, # going into the for loop\n",
    "                                                   batch_size = -1)\n",
    "\n",
    "## Our explicative data is now 'sinkhorn_potentials'.\n",
    "## Then we train_test_split our X and Y data\n",
    "x_train_unchanged, x_test_unchanged, y_train_unchanged, y_test_unchanged = train_test_split(sinkhorn_potentials_unchanged, y, test_size = 0.50, random_state = 42)\n",
    "x_train_self, x_test_self, y_train_self, y_test_self = train_test_split(sinkhorn_potentials_self, y, test_size = 0.50, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sinkhorn_potentials_self.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We perform the Kernel Ridge Regression\n",
    "krr = KernelRidge(kernel = \"rbf\")\n",
    "\n",
    "## Define the parameter grid\n",
    "param_grid = {'alpha': [0.01, 0.1, 0.5, 1], 'gamma': [0.01, 0.1, 0.5, 1]}\n",
    "\n",
    "## Create GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator = krr, \n",
    "                           param_grid = param_grid, \n",
    "                           scoring = 'neg_mean_squared_error', \n",
    "                           cv = 5)\n",
    "\n",
    "## Fit the model ie training the model\n",
    "grid_search.fit(X = x_train_unchanged, y = y_train_unchanged)\n",
    "\n",
    "## Get the best parameters from the cross validation\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "## Get the best model\n",
    "my_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "## Obtain predictions for the test set\n",
    "predictions = my_model.predict(X = x_test_unchanged)\n",
    "\n",
    "## Compute the MSE\n",
    "mse = mean_squared_error(y_true = y_test_unchanged, y_pred = predictions)\n",
    "\n",
    "# Compute the EVS\n",
    "evs = explained_variance_score(y_true = y_test_unchanged, y_pred = predictions)\n",
    "\n",
    "# Print the MSE and EVS\n",
    "print(f'Mean Square Error on the Test Set: {mse}')\n",
    "print(f'Explained Variance Score on the Test Set: {evs}')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "sns.set_theme(context='paper', font_scale=1.5)\n",
    "sns.scatterplot(x=y_test_unchanged, y=predictions, color='blue', alpha=0.5)\n",
    "\n",
    "## Adding the x=y line and the text\n",
    "plt.plot([min(y_test_unchanged), max(y_test_unchanged)], [min(y_test_unchanged), max(y_test_unchanged)], linestyle='--', color='red', linewidth=2, alpha = 0.8)\n",
    "plt.text(min(y_test_unchanged), max(predictions), f'EVS = {evs:.3f}', ha='left', va='top', color='black', fontsize=10, weight='bold')\n",
    "\n",
    "plt.title('Predicted vs. Actual Values of Y')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "\n",
    "#plt.savefig('Images/regression_toy_experiment.png', dpi=300, bbox_inches='tight',format=\"png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We perform the Kernel Ridge Regression\n",
    "krr = KernelRidge(kernel = \"rbf\")\n",
    "\n",
    "## Define the parameter grid\n",
    "param_grid = {'alpha': [0.01, 0.1, 0.5, 1], 'gamma': [0.01, 0.1, 0.5, 1]}\n",
    "\n",
    "## Create GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator = krr, \n",
    "                           param_grid = param_grid, \n",
    "                           scoring = 'neg_mean_squared_error', \n",
    "                           cv = 5)\n",
    "\n",
    "## Fit the model ie training the model\n",
    "grid_search.fit(X = x_train_self, y = y_train_self)\n",
    "\n",
    "## Get the best parameters from the cross validation\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "## Get the best model\n",
    "my_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "## Obtain predictions for the test set\n",
    "predictions = my_model.predict(X = x_test_self)\n",
    "\n",
    "## Compute the MSE\n",
    "mse = mean_squared_error(y_true = y_test_self, y_pred = predictions)\n",
    "\n",
    "# Compute the EVS\n",
    "evs = explained_variance_score(y_true = y_test_self, y_pred = predictions)\n",
    "\n",
    "# Print the MSE and EVS\n",
    "print(f'Mean Square Error on the Test Set: {mse}')\n",
    "print(f'Explained Variance Score on the Test Set: {evs}')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "sns.set_theme(context='paper', font_scale=1.5)\n",
    "sns.scatterplot(x=y_test_self, y=predictions, color='blue', alpha=0.5)\n",
    "\n",
    "## Adding the x=y line and the text\n",
    "plt.plot([min(y_test_self), max(y_test_self)], [min(y_test_self), max(y_test_self)], linestyle='--', color='red', linewidth=2, alpha = 0.8)\n",
    "plt.text(min(y_test_self), max(predictions), f'EVS = {evs:.3f}', ha='left', va='top', color='black', fontsize=10, weight='bold')\n",
    "\n",
    "plt.title('Predicted vs. Actual Values of Y')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "\n",
    "#plt.savefig('Images/regression_toy_experiment.png', dpi=300, bbox_inches='tight',format=\"png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be performing the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing on the real dataset\n",
    "\n",
    "Using as single feature the blade's meshes is not convincing. We refer to \"experiment_300_001\" to see the rubbishness.\n",
    "\n",
    "Exploring the Rotor37 dataset is the way to go and incorpore the omega and P values in the input matrix. Then using a kernel on those scalars inputs and the sinkhorn kernel fpr the meshes and than using the product of those two kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels.RBF(length_scale = np.array([1, 1, 1]), length_scale_bounds=(1e-05, 100000.0))\n",
    "kernels.Product()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of blade one want to consider.\n",
    "_many_blades = 100\n",
    "\n",
    "## Creating the list of all file numbers.\n",
    "padded_numbers = [str(i).zfill(9) for i in range(_many_blades)]\n",
    "\n",
    "## Lists that will holds the cloud points and the associated efficiency.\n",
    "distributions = []\n",
    "efficiency = []\n",
    "omega = []\n",
    "P = []\n",
    "\n",
    "for number in padded_numbers:\n",
    "    ## File paths Google Colab\n",
    "    #cgns_file_path = f'/content/drive/MyDrive/Developer/GraduationProject/Experimental Part/Rotor37/dataset/samples/sample_{number}/meshes/mesh_000000000.cgns'\n",
    "    #coefficient_file_path = f'/content/drive/MyDrive/Developer/GraduationProject/Experimental Part/Rotor37/dataset/samples/sample_{number}/scalars.csv'\n",
    "    ## File paths Personal Computer\n",
    "    cgns_file_path = f'Rotor37/dataset/samples/sample_{number}/meshes/mesh_000000000.cgns'\n",
    "    coefficient_file_path = f'Rotor37/dataset/samples/sample_{number}/scalars.csv'\n",
    "    ## Computing the coordinates\n",
    "    x, y, z = read_cgns_coordinates(cgns_file_path)\n",
    "    blade = np.column_stack((x, y, z))\n",
    "    ## Computing the coefficient\n",
    "    scalars = pd.read_csv(coefficient_file_path)\n",
    "    ## Adding to our data\n",
    "    distributions.append(blade)\n",
    "    efficiency.append(scalars[\"Efficiency\"][0])\n",
    "    omega.append(scalars[\"Omega\"][0])\n",
    "    P.append(scalars[\"P\"][0])\n",
    "\n",
    "## Train Test split of 70%\n",
    "#x_train, x_test, y_train, y_test = train_test_split(distributions, efficiency, train_size = 0.7, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = np.array(omega)\n",
    "P = np.array(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = P.reshape(-1, 1)\n",
    "omega = omega.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((sinkhorn_potentials_self, P, omega))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.66916885e+01, -6.79214325e+01, -6.76675339e+01,\n",
       "        -6.77080994e+01, -6.74003525e+01, -6.79887390e+01,\n",
       "         3.66255350e+05,  1.64151000e+03],\n",
       "       [-6.69345245e+01, -6.77611084e+01, -6.75654068e+01,\n",
       "        -6.75487518e+01, -6.74830246e+01, -6.80257568e+01,\n",
       "         3.72627750e+05,  1.63953000e+03],\n",
       "       [-6.75763474e+01, -6.78945236e+01, -6.79334488e+01,\n",
       "        -6.78927689e+01, -6.79251175e+01, -6.78454590e+01,\n",
       "         3.64912850e+05,  1.74861000e+03],\n",
       "       [-6.68796539e+01, -6.80120239e+01, -6.78323517e+01,\n",
       "        -6.78783722e+01, -6.75477905e+01, -6.79064789e+01,\n",
       "         3.61225450e+05,  1.62513000e+03],\n",
       "       [-6.69021835e+01, -6.78440399e+01, -6.76346970e+01,\n",
       "        -6.76371384e+01, -6.74937210e+01, -6.80388794e+01,\n",
       "         3.69387850e+05,  1.66581000e+03],\n",
       "       [-6.67251434e+01, -6.79902115e+01, -6.77831192e+01,\n",
       "        -6.78413620e+01, -6.74533768e+01, -6.78541489e+01,\n",
       "         3.72735150e+05,  1.63467000e+03],\n",
       "       [-6.72494583e+01, -6.76229172e+01, -6.75386276e+01,\n",
       "        -6.74736404e+01, -6.76146088e+01, -6.79503784e+01,\n",
       "         3.75151650e+05,  1.79901000e+03],\n",
       "       [-6.75597763e+01, -6.79060516e+01, -6.79311295e+01,\n",
       "        -6.78942947e+01, -6.79151535e+01, -6.78663177e+01,\n",
       "         3.67383050e+05,  1.78263000e+03],\n",
       "       [-6.72055969e+01, -6.79498749e+01, -6.78445587e+01,\n",
       "        -6.78393173e+01, -6.77228775e+01, -6.79760818e+01,\n",
       "         3.71124150e+05,  1.70217000e+03],\n",
       "       [-6.73879547e+01, -6.78554688e+01, -6.78070068e+01,\n",
       "        -6.77687531e+01, -6.77932739e+01, -6.79587097e+01,\n",
       "         3.73379550e+05,  1.64601000e+03],\n",
       "       [-6.74328613e+01, -6.77294388e+01, -6.76995468e+01,\n",
       "        -6.76369858e+01, -6.77667313e+01, -6.79362411e+01,\n",
       "         3.73970250e+05,  1.64133000e+03],\n",
       "       [-6.73726807e+01, -6.79313507e+01, -6.78777237e+01,\n",
       "        -6.78556519e+01, -6.78127213e+01, -6.79571457e+01,\n",
       "         3.64035750e+05,  1.69551000e+03],\n",
       "       [-6.68684082e+01, -6.79099960e+01, -6.76919327e+01,\n",
       "        -6.77105331e+01, -6.74977341e+01, -6.80247955e+01,\n",
       "         3.75080050e+05,  1.71693000e+03],\n",
       "       [-6.72487640e+01, -6.79775391e+01, -6.78903427e+01,\n",
       "        -6.78886261e+01, -6.77578812e+01, -6.79506760e+01,\n",
       "         3.65109750e+05,  1.76751000e+03],\n",
       "       [-6.75187836e+01, -6.76931458e+01, -6.76981277e+01,\n",
       "        -6.76244049e+01, -6.78021698e+01, -6.78966599e+01,\n",
       "         3.74131350e+05,  1.75527000e+03],\n",
       "       [-6.74219513e+01, -6.77441101e+01, -6.77075653e+01,\n",
       "        -6.76480789e+01, -6.77659149e+01, -6.79472656e+01,\n",
       "         3.71750650e+05,  1.64025000e+03],\n",
       "       [-6.67454681e+01, -6.79922714e+01, -6.77814102e+01,\n",
       "        -6.78370743e+01, -6.74627686e+01, -6.78779907e+01,\n",
       "         3.73862850e+05,  1.69875000e+03],\n",
       "       [-6.65506592e+01, -6.76107254e+01, -6.73087463e+01,\n",
       "        -6.73062668e+01, -6.71862106e+01, -6.80083923e+01,\n",
       "         3.66631250e+05,  1.63053000e+03],\n",
       "       [-6.76028290e+01, -6.78398895e+01, -6.78760300e+01,\n",
       "        -6.78223190e+01, -6.79129105e+01, -6.78733139e+01,\n",
       "         3.60867450e+05,  1.79649000e+03],\n",
       "       [-6.71402740e+01, -6.77771454e+01, -6.76367874e+01,\n",
       "        -6.76052094e+01, -6.76078415e+01, -6.80278015e+01,\n",
       "         3.69065650e+05,  1.64763000e+03],\n",
       "       [-6.72809143e+01, -6.79945526e+01, -6.79220352e+01,\n",
       "        -6.79230118e+01, -6.77825317e+01, -6.79250183e+01,\n",
       "         3.70032250e+05,  1.64637000e+03],\n",
       "       [-6.68170929e+01, -6.80097961e+01, -6.78156967e+01,\n",
       "        -6.78674545e+01, -6.75100632e+01, -6.78920212e+01,\n",
       "         3.73236350e+05,  1.64385000e+03],\n",
       "       [-6.70754471e+01, -6.79762573e+01, -6.78341904e+01,\n",
       "        -6.78466110e+01, -6.76551895e+01, -6.79747391e+01,\n",
       "         3.62657450e+05,  1.76859000e+03],\n",
       "       [-6.72047043e+01, -6.79492035e+01, -6.78408813e+01,\n",
       "        -6.78356781e+01, -6.77204971e+01, -6.79828949e+01,\n",
       "         3.71643250e+05,  1.65285000e+03],\n",
       "       [-6.74264145e+01, -6.77751465e+01, -6.77427979e+01,\n",
       "        -6.76878128e+01, -6.77841721e+01, -6.79433136e+01,\n",
       "         3.68940350e+05,  1.74285000e+03],\n",
       "       [-6.65810165e+01, -6.76060791e+01, -6.73154755e+01,\n",
       "        -6.73091278e+01, -6.72037277e+01, -6.80014572e+01,\n",
       "         3.75724450e+05,  1.67373000e+03],\n",
       "       [-6.64730377e+01, -6.76300201e+01, -6.73054581e+01,\n",
       "        -6.73141022e+01, -6.71470871e+01, -6.80093613e+01,\n",
       "         3.64572750e+05,  1.75833000e+03],\n",
       "       [-6.66220322e+01, -6.78257217e+01, -6.75441360e+01,\n",
       "        -6.75713043e+01, -6.73199997e+01, -6.80144348e+01,\n",
       "         3.67186150e+05,  1.77273000e+03],\n",
       "       [-6.75648804e+01, -6.79295349e+01, -6.79645081e+01,\n",
       "        -6.79329147e+01, -6.79293289e+01, -6.78446732e+01,\n",
       "         3.63480850e+05,  1.73007000e+03],\n",
       "       [-6.74603729e+01, -6.79674988e+01, -6.79686737e+01,\n",
       "        -6.79522858e+01, -6.78823242e+01, -6.78602448e+01,\n",
       "         3.65915250e+05,  1.67121000e+03],\n",
       "       [-6.74329529e+01, -6.78770752e+01, -6.78418045e+01,\n",
       "        -6.78037949e+01, -6.78271179e+01, -6.79575882e+01,\n",
       "         3.62174150e+05,  1.73763000e+03],\n",
       "       [-6.71359100e+01, -6.76747208e+01, -6.75430908e+01,\n",
       "        -6.74955597e+01, -6.75649033e+01, -6.79942551e+01,\n",
       "         3.70264950e+05,  1.74195000e+03],\n",
       "       [-6.71942291e+01, -6.79620285e+01, -6.78541565e+01,\n",
       "        -6.78534012e+01, -6.77195511e+01, -6.79781265e+01,\n",
       "         3.60724250e+05,  1.64277000e+03],\n",
       "       [-6.69261246e+01, -6.79983749e+01, -6.78268738e+01,\n",
       "        -6.78625031e+01, -6.75739822e+01, -6.79310379e+01,\n",
       "         3.61762450e+05,  1.67553000e+03],\n",
       "       [-6.66920929e+01, -6.79803085e+01, -6.77562943e+01,\n",
       "        -6.78147507e+01, -6.74272766e+01, -6.78768616e+01,\n",
       "         3.76440450e+05,  1.74843000e+03],\n",
       "       [-6.73151550e+01, -6.76637650e+01, -6.75948639e+01,\n",
       "        -6.75312195e+01, -6.76686478e+01, -6.79581680e+01,\n",
       "         3.73791250e+05,  1.72467000e+03],\n",
       "       [-6.69313736e+01, -6.80137634e+01, -6.78602753e+01,\n",
       "        -6.79016953e+01, -6.75854340e+01, -6.78745804e+01,\n",
       "         3.71607450e+05,  1.62909000e+03],\n",
       "       [-6.67996826e+01, -6.77800217e+01, -6.75443954e+01,\n",
       "        -6.75444794e+01, -6.74083557e+01, -6.80288086e+01,\n",
       "         3.65414050e+05,  1.77615000e+03],\n",
       "       [-6.74119492e+01, -6.76155624e+01, -6.75874329e+01,\n",
       "        -6.75090866e+01, -6.77070312e+01, -6.79150009e+01,\n",
       "         3.61458150e+05,  1.71045000e+03],\n",
       "       [-6.75576706e+01, -6.78238602e+01, -6.78439865e+01,\n",
       "        -6.77892303e+01, -6.78824768e+01, -6.78897858e+01,\n",
       "         3.68922450e+05,  1.67661000e+03],\n",
       "       [-6.68479996e+01, -6.76878586e+01, -6.74629517e+01,\n",
       "        -6.74440842e+01, -6.73939514e+01, -6.80267258e+01,\n",
       "         3.59220650e+05,  1.68435000e+03],\n",
       "       [-6.76088333e+01, -6.78191833e+01, -6.78539200e+01,\n",
       "        -6.77962112e+01, -6.79059753e+01, -6.78813705e+01,\n",
       "         3.61816150e+05,  1.79091000e+03],\n",
       "       [-6.74167175e+01, -6.79080963e+01, -6.78721008e+01,\n",
       "        -6.78420105e+01, -6.78315811e+01, -6.79404831e+01,\n",
       "         3.58952150e+05,  1.67337000e+03],\n",
       "       [-6.69012756e+01, -6.77651443e+01, -6.75572891e+01,\n",
       "        -6.75451736e+01, -6.74622803e+01, -6.80350342e+01,\n",
       "         3.65378250e+05,  1.65231000e+03],\n",
       "       [-6.69747772e+01, -6.76541595e+01, -6.74763260e+01,\n",
       "        -6.74389877e+01, -6.74622498e+01, -6.80020828e+01,\n",
       "         3.67776850e+05,  1.78929000e+03],\n",
       "       [-6.70540237e+01, -6.80141754e+01, -6.78722687e+01,\n",
       "        -6.78982391e+01, -6.76536484e+01, -6.79425583e+01,\n",
       "         3.76028750e+05,  1.64421000e+03],\n",
       "       [-6.69837875e+01, -6.77546310e+01, -6.75687256e+01,\n",
       "        -6.75467911e+01, -6.75056686e+01, -6.80325699e+01,\n",
       "         3.64536950e+05,  1.74573000e+03],\n",
       "       [-6.70349274e+01, -6.77445908e+01, -6.75748825e+01,\n",
       "        -6.75472031e+01, -6.75317307e+01, -6.80327225e+01,\n",
       "         3.61386550e+05,  1.63917000e+03],\n",
       "       [-6.72496262e+01, -6.78632812e+01, -6.77592392e+01,\n",
       "        -6.77331238e+01, -6.77101440e+01, -6.80129776e+01,\n",
       "         3.75259050e+05,  1.72485000e+03],\n",
       "       [-6.76408310e+01, -6.79028244e+01, -6.79741821e+01,\n",
       "        -6.79356155e+01, -6.79605484e+01, -6.77880402e+01,\n",
       "         3.70300750e+05,  1.70325000e+03],\n",
       "       [-6.71349564e+01, -6.80192108e+01, -6.79163437e+01,\n",
       "        -6.79380112e+01, -6.77070694e+01, -6.78920288e+01,\n",
       "         3.74686250e+05,  1.62225000e+03],\n",
       "       [-6.72218246e+01, -6.79450073e+01, -6.78416443e+01,\n",
       "        -6.78347092e+01, -6.77284241e+01, -6.79834518e+01,\n",
       "         3.66541750e+05,  1.62585000e+03],\n",
       "       [-6.71865845e+01, -6.75862885e+01, -6.74783707e+01,\n",
       "        -6.74132614e+01, -6.75570526e+01, -6.79603882e+01,\n",
       "         3.74381950e+05,  1.68975000e+03],\n",
       "       [-6.69129944e+01, -6.79224396e+01, -6.77244949e+01,\n",
       "        -6.77412720e+01, -6.75345459e+01, -6.80087051e+01,\n",
       "         3.60384150e+05,  1.69929000e+03],\n",
       "       [-6.76342468e+01, -6.78426208e+01, -6.78981018e+01,\n",
       "        -6.78436279e+01, -6.79357452e+01, -6.78369675e+01,\n",
       "         3.75294850e+05,  1.72557000e+03],\n",
       "       [-6.75517120e+01, -6.78662262e+01, -6.78809204e+01,\n",
       "        -6.78347397e+01, -6.78943100e+01, -6.78986816e+01,\n",
       "         3.75939250e+05,  1.73367000e+03],\n",
       "       [-6.65721436e+01, -6.76375656e+01, -6.73430176e+01,\n",
       "        -6.73426666e+01, -6.72117233e+01, -6.80121384e+01,\n",
       "         3.64877050e+05,  1.77057000e+03],\n",
       "       [-6.66722260e+01, -6.78158417e+01, -6.75422134e+01,\n",
       "        -6.75620270e+01, -6.73437958e+01, -6.80287781e+01,\n",
       "         3.75491750e+05,  1.65321000e+03],\n",
       "       [-6.73059845e+01, -6.76384735e+01, -6.75680618e+01,\n",
       "        -6.75014343e+01, -6.76511078e+01, -6.79510727e+01,\n",
       "         3.69566850e+05,  1.78911000e+03],\n",
       "       [-6.69088821e+01, -6.76563644e+01, -6.74593048e+01,\n",
       "        -6.74284973e+01, -6.74240494e+01, -6.80048752e+01,\n",
       "         3.71052550e+05,  1.69425000e+03],\n",
       "       [-6.65510712e+01, -6.77140579e+01, -6.74109421e+01,\n",
       "        -6.74255676e+01, -6.72302475e+01, -6.80170822e+01,\n",
       "         3.70837750e+05,  1.68363000e+03],\n",
       "       [-6.75909653e+01, -6.78360977e+01, -6.78638077e+01,\n",
       "        -6.78101578e+01, -6.79028625e+01, -6.78868179e+01,\n",
       "         3.73737550e+05,  1.71603000e+03],\n",
       "       [-6.73269043e+01, -6.79565659e+01, -6.78959885e+01,\n",
       "        -6.78831940e+01, -6.77982330e+01, -6.79477463e+01,\n",
       "         3.61064350e+05,  1.68237000e+03],\n",
       "       [-6.65851822e+01, -6.77127075e+01, -6.74183121e+01,\n",
       "        -6.74289093e+01, -6.72505035e+01, -6.80216827e+01,\n",
       "         3.72699350e+05,  1.77345000e+03],\n",
       "       [-6.72916718e+01, -6.80096130e+01, -6.79544830e+01,\n",
       "        -6.79612808e+01, -6.77958221e+01, -6.78767700e+01,\n",
       "         3.74507250e+05,  1.77867000e+03],\n",
       "       [-6.74605484e+01, -6.78463135e+01, -6.78231888e+01,\n",
       "        -6.77778168e+01, -6.78320465e+01, -6.79449615e+01,\n",
       "         3.69262550e+05,  1.71117000e+03],\n",
       "       [-6.74530640e+01, -6.79626312e+01, -6.79492340e+01,\n",
       "        -6.79319458e+01, -6.78715134e+01, -6.78942032e+01,\n",
       "         3.66971350e+05,  1.74267000e+03],\n",
       "       [-6.70980453e+01, -6.75821686e+01, -6.74481888e+01,\n",
       "        -6.73905334e+01, -6.75037613e+01, -6.79660263e+01,\n",
       "         3.71517950e+05,  1.68615000e+03],\n",
       "       [-6.69342575e+01, -6.78832016e+01, -6.76862717e+01,\n",
       "        -6.76923676e+01, -6.75306549e+01, -6.80270996e+01,\n",
       "         3.73415350e+05,  1.66275000e+03],\n",
       "       [-6.66697693e+01, -6.79225082e+01, -6.76619110e+01,\n",
       "        -6.77049332e+01, -6.73866272e+01, -6.79776688e+01,\n",
       "         3.73630150e+05,  1.62171000e+03],\n",
       "       [-6.71305695e+01, -6.75505753e+01, -6.74312286e+01,\n",
       "        -6.73657761e+01, -6.75107269e+01, -6.79533691e+01,\n",
       "         3.64608550e+05,  1.77093000e+03],\n",
       "       [-6.73634415e+01, -6.78440170e+01, -6.77806244e+01,\n",
       "        -6.77420807e+01, -6.77709656e+01, -6.79828568e+01,\n",
       "         3.62013050e+05,  1.75311000e+03],\n",
       "       [-6.76479340e+01, -6.78190079e+01, -6.78751068e+01,\n",
       "        -6.78160629e+01, -6.79309769e+01, -6.78417206e+01,\n",
       "         3.66953450e+05,  1.66149000e+03],\n",
       "       [-6.71368408e+01, -6.78390045e+01, -6.76915970e+01,\n",
       "        -6.76705551e+01, -6.76269226e+01, -6.80391235e+01,\n",
       "         3.64161050e+05,  1.75149000e+03],\n",
       "       [-6.74985352e+01, -6.79545822e+01, -6.79690247e+01,\n",
       "        -6.79463348e+01, -6.79012604e+01, -6.78532104e+01,\n",
       "         3.65181350e+05,  1.74015000e+03],\n",
       "       [-6.71853027e+01, -6.75904846e+01, -6.74870834e+01,\n",
       "        -6.74226227e+01, -6.75619736e+01, -6.79571686e+01,\n",
       "         3.73307950e+05,  1.72719000e+03],\n",
       "       [-6.72951736e+01, -6.79866943e+01, -6.79198532e+01,\n",
       "        -6.79183044e+01, -6.77892838e+01, -6.79284515e+01,\n",
       "         3.67579950e+05,  1.79397000e+03],\n",
       "       [-6.71257095e+01, -6.75243225e+01, -6.74074936e+01,\n",
       "        -6.73388214e+01, -6.74976501e+01, -6.79371490e+01,\n",
       "         3.59310150e+05,  1.79271000e+03],\n",
       "       [-6.67855377e+01, -6.78013382e+01, -6.75574341e+01,\n",
       "        -6.75635452e+01, -6.74042206e+01, -6.80375977e+01,\n",
       "         3.76529950e+05,  1.79937000e+03],\n",
       "       [-6.70133209e+01, -6.79864960e+01, -6.78232727e+01,\n",
       "        -6.78454590e+01, -6.76171494e+01, -6.79789810e+01,\n",
       "         3.64322150e+05,  1.71153000e+03],\n",
       "       [-6.70789642e+01, -6.79434814e+01, -6.77897034e+01,\n",
       "        -6.77941437e+01, -6.76394806e+01, -6.80122833e+01,\n",
       "         3.61690850e+05,  1.73817000e+03],\n",
       "       [-6.70587540e+01, -6.78140030e+01, -6.76543121e+01,\n",
       "        -6.76357193e+01, -6.75786591e+01, -6.80239868e+01,\n",
       "         3.70426050e+05,  1.77939000e+03],\n",
       "       [-6.69053192e+01, -6.80053864e+01, -6.78317871e+01,\n",
       "        -6.78715973e+01, -6.75640640e+01, -6.79102173e+01,\n",
       "         3.70963050e+05,  1.65357000e+03],\n",
       "       [-6.75143661e+01, -6.78632050e+01, -6.78662338e+01,\n",
       "        -6.78212204e+01, -6.78735046e+01, -6.79076385e+01,\n",
       "         3.75438050e+05,  1.79055000e+03],\n",
       "       [-6.71876450e+01, -6.80146637e+01, -6.79272919e+01,\n",
       "        -6.79425659e+01, -6.77377014e+01, -6.78969193e+01,\n",
       "         3.75205350e+05,  1.76031000e+03],\n",
       "       [-6.74949265e+01, -6.76367416e+01, -6.76433105e+01,\n",
       "        -6.75624847e+01, -6.77684250e+01, -6.78743362e+01,\n",
       "         3.67114550e+05,  1.62423000e+03],\n",
       "       [-6.74207458e+01, -6.78097153e+01, -6.77717819e+01,\n",
       "        -6.77233963e+01, -6.77927017e+01, -6.79520187e+01,\n",
       "         3.72305550e+05,  1.75095000e+03],\n",
       "       [-6.73998795e+01, -6.77322083e+01, -6.76883469e+01,\n",
       "        -6.76286316e+01, -6.77474060e+01, -6.79537659e+01,\n",
       "         3.63427150e+05,  1.62099000e+03],\n",
       "       [-6.71805954e+01, -6.77572861e+01, -6.76367645e+01,\n",
       "        -6.75981293e+01, -6.76268005e+01, -6.80045853e+01,\n",
       "         3.73522750e+05,  1.73187000e+03],\n",
       "       [-6.72452850e+01, -6.75209808e+01, -6.74454422e+01,\n",
       "        -6.73669510e+01, -6.75661240e+01, -6.79110260e+01,\n",
       "         3.66792350e+05,  1.73997000e+03],\n",
       "       [-6.75671539e+01, -6.78294296e+01, -6.78544922e+01,\n",
       "        -6.78004532e+01, -6.78914642e+01, -6.78825455e+01,\n",
       "         3.74704150e+05,  1.63719000e+03],\n",
       "       [-6.64824524e+01, -6.76230545e+01, -6.73084106e+01,\n",
       "        -6.73145905e+01, -6.71544876e+01, -6.79885406e+01,\n",
       "         3.71446350e+05,  1.75815000e+03],\n",
       "       [-6.70894165e+01, -6.79823608e+01, -6.78377380e+01,\n",
       "        -6.78501587e+01, -6.76618652e+01, -6.79917679e+01,\n",
       "         3.64340050e+05,  1.70847000e+03],\n",
       "       [-6.69545822e+01, -6.77156754e+01, -6.75257263e+01,\n",
       "        -6.75004807e+01, -6.74741135e+01, -6.80232849e+01,\n",
       "         3.65217150e+05,  1.64709000e+03],\n",
       "       [-6.74014282e+01, -6.78038940e+01, -6.77590637e+01,\n",
       "        -6.77110138e+01, -6.77789154e+01, -6.79596329e+01,\n",
       "         3.69620550e+05,  1.77777000e+03],\n",
       "       [-6.68222504e+01, -6.80007401e+01, -6.78003616e+01,\n",
       "        -6.78486557e+01, -6.75083618e+01, -6.79131622e+01,\n",
       "         3.61798250e+05,  1.79469000e+03],\n",
       "       [-6.68064194e+01, -6.76583862e+01, -6.74253006e+01,\n",
       "        -6.74052353e+01, -6.73590088e+01, -6.80253372e+01,\n",
       "         3.72466650e+05,  1.71315000e+03],\n",
       "       [-6.73671265e+01, -6.79067688e+01, -6.78527374e+01,\n",
       "        -6.78261032e+01, -6.78015900e+01, -6.79579849e+01,\n",
       "         3.67651550e+05,  1.72323000e+03],\n",
       "       [-6.65983200e+01, -6.77585449e+01, -6.74674988e+01,\n",
       "        -6.74847107e+01, -6.72779617e+01, -6.80218353e+01,\n",
       "         3.60939050e+05,  1.67535000e+03],\n",
       "       [-6.66319046e+01, -6.76629944e+01, -6.73845596e+01,\n",
       "        -6.73819962e+01, -6.72597656e+01, -6.80104904e+01,\n",
       "         3.76046650e+05,  1.66941000e+03]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = distributions[0]\n",
    "\n",
    "## Convert the list of points mu into a WeightedPointCloud object\n",
    "mu_cloud = WeightedPointCloud(\n",
    "    cloud=jnp.array(mu),\n",
    "    weights=jnp.ones(len(mu))\n",
    ")\n",
    "\n",
    "## First we convert the list all the sampled distributions to WeightedPointCloud objects\n",
    "list_of_weighted_point_clouds = []\n",
    "for sample in distributions:\n",
    "    distrib_cloud = WeightedPointCloud(\n",
    "        cloud=jnp.array(sample),\n",
    "        weights=jnp.ones(len(sample))\n",
    "    )\n",
    "    list_of_weighted_point_clouds.append(distrib_cloud)\n",
    "\n",
    "## We need to convert the cloud list to a VectorizedWeightedPointCloud\n",
    "x_cloud = pad_point_clouds(list_of_weighted_point_clouds)\n",
    "\n",
    "## We choose our epsilon parameter and perform the sinkhirn algorithm\n",
    "sinkhorn_solver_kwargs = {'epsilon': 1}\n",
    "sinkhorn_potentials = clouds_to_dual_sinkhorn(points = x_cloud, mu = mu_cloud, \n",
    "                                                   sinkhorn_solver_kwargs = sinkhorn_solver_kwargs, \n",
    "                                                   parallel = False, # going into the for loop\n",
    "                                                   batch_size = -1)\n",
    "\n",
    "## Preprocessing step\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Define the kernels for different parts of the feature matrix\n",
    "kernel_sinkhorn = kernels.RBF()\n",
    "kernel_scalars = kernels.RBF()\n",
    "\n",
    "# Define the column transformer to apply different kernels to different columns\n",
    "transformers = [\n",
    "    ('blade', FunctionTransformer(None), slice(0, 6)),  # No transformation for mesh\n",
    "    ('scalars', FunctionTransformer(None), slice(6, 8)),  # No transformation for P and omega\n",
    "    ('kernel_sinkhorn', kernel_sinkhorn, slice(0, 6)),  # Kernel for mesh\n",
    "    ('kernel_scalars', kernel_scalars, slice(6, 8)),  # Kernel for P and omega\n",
    "]\n",
    "\n",
    "# Apply the column transformer to the feature matrix The X_transformed matrix is now the K matrix ?\n",
    "ct = ColumnTransformer(transformers)\n",
    "K = ct.fit_transform(X)\n",
    "\n",
    "\n",
    "## Our explicative data is now 'sinkhorn_potentials'.\n",
    "## Then we train_test_split our X and Y data\n",
    "x_train, x_test, y_train, y_test = train_test_split(sinkhorn_potentials, efficiency, train_size = 0.7, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_x3 = kernels.RBF(length_scale = np.array([1, 1, 1]), length_scale_bounds=(1e-05, 100000.0))\n",
    "matern = kernels.Matern(length_scale = np.array([1, 1, 1]), length_scale_bounds=(1e-05, 100000.0))\n",
    "\n",
    "my_kernel = kernels.Product(rbf_x3, matern)\n",
    "## We perform the Kernel Ridge Regression\n",
    "krr = KernelRidge(kernel = rbf_x3)\n",
    "\n",
    "## Define the parameter grid\n",
    "param_grid = {'alpha': [0.001, 0.01, 0.1, 0.5, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 0.5, 1, 10, 100]}\n",
    "\n",
    "## Create GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator = krr, \n",
    "                           param_grid = param_grid, \n",
    "                           scoring = 'neg_mean_squared_error', \n",
    "                           cv = 5)\n",
    "\n",
    "## Fit the model ie training the model\n",
    "grid_search.fit(X = x_train, y = y_train)\n",
    "\n",
    "## Get the best parameters from the cross validation\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "## Get the best model\n",
    "my_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "## Obtain predictions for the test set\n",
    "predictions = my_model.predict(X = x_test)\n",
    "\n",
    "## Compute the MSE\n",
    "mse = mean_squared_error(y_true = y_test, y_pred = predictions)\n",
    "\n",
    "# Compute the EVS\n",
    "evs = explained_variance_score(y_true = y_test, y_pred = predictions)\n",
    "\n",
    "# Print the MSE and EVS\n",
    "print(f'Mean Square Error on the Test Set: {mse}')\n",
    "print(f'Explained Variance Score on the Test Set: {evs}')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "sns.set_theme(context='paper', font_scale=1.5)\n",
    "sns.scatterplot(x=y_test, y=predictions, color='blue', alpha=0.5)\n",
    "\n",
    "## Adding the x=y line and the text\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2, alpha = 0.8)\n",
    "plt.text(min(y_test), max(predictions), f'EVS = {evs:.3f}', ha='left', va='top', color='black', fontsize=10, weight='bold')\n",
    "\n",
    "plt.title('Predicted vs. Actual Values of Y')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "\n",
    "#plt.savefig('Images/regression_toy_experiment.png', dpi=300, bbox_inches='tight',format=\"png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
